@ROUT ATL_trsmKL_rk4 ATL_trsmKR_rk4 ATL_ctrsmKL_rk2 ATL_ctrsmKR_rk2
#include "atlas_misc.h"
#include "atlas_prefetch.h"
#define RTYPE register TYPE

#if defined(__GNUC__) || \
    (defined(__STDC_VERSION__) && (__STDC_VERSION__/100 >= 1999))
   #define ATL_SINLINE static inline
#else
   #define ATL_SINLINE static
#endif
@ROUT ATL_ctrsmKL_rk2 ATL_ctrsmKR_rk2
#if defined(ATL_AVX) && defined(DCPLX)
   #define NRHS 3
   #define ATL_BINWRK 1
   #include <immintrin.h>
/*
 * Subtract off x0 & x1 contribution to all remaining equations using a
 * rank-2 update with mu=2, nu=3, ku=2.  This version is for 16 AVX regs.
 * nu is the # of RHS, ku is the number of equations solved, and mu is 
 * unrolled only to enable vectorization & software pipelining of load/use.
 * Loop order is MKN, so that B is kept completely in registers, and
 * C and A are streamed in (and out, for C) from cache during the operation.
 */
ATL_SINLINE void ATL_rk2(ATL_CINT M, const TYPE *pA0, ATL_CINT lda0, 
                           TYPE *pB0, ATL_CINT ldb0, TYPE *C, ATL_CINT ldc0) 
{
   ATL_CINT lda=lda0+lda0, ldb=ldb0+ldb0, ldc=ldc0+ldc0;
   const TYPE *pA1 = pA0+lda; 
   TYPE *pC0 = C, *pC1 = C+ldc, *pC2 = C+((ldc)<<1);
   TYPE *pB2 = pB0 + (ldb<<1);
   ATL_CINT MM =  (M & 2) ? M-2 : M-4; 
   int i; 
   register __m256d rB00, iB00, rB01, iB01, rB02, iB02;
   register __m256d C00, C01, C02; 
   register __m256d C20, C21, C22; 
   register __m256d A, a;

   a = _mm256_set1_pd(ATL_rnone);
   rB00 = _mm256_set_pd(pB0[2], *pB0, pB0[2], *pB0);  
                                                /* rB10 rB00 rB10 rB00 */
   rB00 = _mm256_mul_pd(a, rB00);                 
                                                /* negate B for alpha=-1 */
   iB00 = _mm256_set_pd(pB0[3], pB0[1], pB0[3], pB0[1]);
                                                /* iB10 iB00 iB10 iB00 */
   iB00 = _mm256_mul_pd(a, iB00);                   
                                                /* negate B for alpha=-1 */

   rB01 = _mm256_set_pd(pB0[ldb+2], pB0[ldb], pB0[ldb+2], pB0[ldb]);  
                                                /* rB10 rB00 rB10 rB00 */
   rB01 = _mm256_mul_pd(a, rB01);
                                                /* negate B for alpha=-1 */
   iB01 = _mm256_set_pd(pB0[ldb+3], pB0[ldb+1], pB0[ldb+3], pB0[ldb+1]);
                                                /* iB10 iB00 iB10 iB00 */
   iB01 = _mm256_mul_pd(a, iB01);               /* negate B for alpha=-1 */
                                                
   rB02 = _mm256_set_pd(pB2[2], *pB2, pB2[2], *pB2);  
                                                /* rB12 rB02 rB12 rB02 */
   rB02 = _mm256_mul_pd(a, rB02);                 
                                                /* negate B for alpha=-1 */
   iB02 = _mm256_set_pd(pB2[3], pB2[1], pB2[3], pB2[1]);
                                                /* iB10 iB00 iB10 iB00 */
   iB02 = _mm256_mul_pd(a, iB02);                   
                                                /* negate B for alpha=-1 */

   C00  = _mm256_load_pd(pC0);                  /* iC10 rC10 iC00 rC00 */
   C01  = _mm256_load_pd(pC1);                  /* iC11 rC11 iC01 rC01 */
   C02  = _mm256_load_pd(pC2);                  /* iC12 rC12 iC02 rC02 */

   A    = _mm256_load_pd(pA0);                  /* iA10 rA10 iA00 rA00 */
   a    = _mm256_shuffle_pd(A, A, 0x5);         /* rA10 iA10 rA00 iA00 */

   for (i=0; i < MM; i += 4, pA0 += 8, pA1 += 8, pC0 += 8, pC1 += 8, pC2 += 8)
   {                                     /* rB00 = rB10 rB00 rB10 rB00 */
                                         /* iB00 = iB10 iB00 iB10 iB00 */
      register __m256d m, b;
/*
 *    Do M=K=0 calcs
 */
      b = _mm256_unpacklo_pd(rB00, rB00);       /* rB00 rB00 rB00 rB00 */
      m    = _mm256_mul_pd(A, b);
      C00 = _mm256_add_pd(m, C00);  C20 = _mm256_load_pd(pC0+4);
      b = _mm256_unpacklo_pd(rB01, rB01);       /* rB01 rB01 rB01 rB01 */
      m    = _mm256_mul_pd(A, b);
      C01 = _mm256_add_pd(m, C01);  C21 = _mm256_load_pd(pC1+4);
      b = _mm256_unpacklo_pd(rB02, rB02);       /* rB02 rB02 rB02 rB02 */
      m    = _mm256_mul_pd(A, b);
      C02 = _mm256_add_pd(m, C02);  A = _mm256_load_pd(pA1);

      b = _mm256_unpacklo_pd(iB00, iB00);       /* iB00 iB00 iB00 iB00 */
      m    = _mm256_mul_pd(a, b);
      C00 = _mm256_addsub_pd(C00, m);  C22 = _mm256_load_pd(pC2+4);
      b = _mm256_unpacklo_pd(iB01, iB01);
      m    = _mm256_mul_pd(a, b);
      C01 = _mm256_addsub_pd(C01, m);
      b = _mm256_unpacklo_pd(iB02, iB02);
      m    = _mm256_mul_pd(a, b);
      C02 = _mm256_addsub_pd(C02, m); a    = _mm256_shuffle_pd(A, A, 0x5);
/*
 *    Do M=0, K=1 calcs
 */
      b = _mm256_unpackhi_pd(rB00, rB00);       /* rB10 rB10 rB10 rB10 */
      m    = _mm256_mul_pd(A, b);
      C00 = _mm256_add_pd(m, C00);
      b = _mm256_unpackhi_pd(rB01, rB01);       /* rB11 rB11 rB11 rB11 */
      m    = _mm256_mul_pd(A, b);
      C01 = _mm256_add_pd(m, C01);
      b = _mm256_unpackhi_pd(rB02, rB02);       /* rB12 rB12 rB12 rB12 */
      m    = _mm256_mul_pd(A, b);
      C02 = _mm256_add_pd(m, C02);  A = _mm256_load_pd(pA0+4);

      b = _mm256_unpackhi_pd(iB00, iB00);       /* iB10 iB10 iB10 iB10 */
      m    = _mm256_mul_pd(a, b);
      C00 = _mm256_addsub_pd(C00, m); _mm256_store_pd(pC0, C00);
      b = _mm256_unpackhi_pd(iB01, iB01);  
      m    = _mm256_mul_pd(a, b);
      C01 = _mm256_addsub_pd(C01, m); _mm256_store_pd(pC1, C01);
      b = _mm256_unpackhi_pd(iB02, iB02); 
      m    = _mm256_mul_pd(a, b);
      C02 = _mm256_addsub_pd(C02, m); _mm256_store_pd(pC2, C02);
/*
 *    Do M=2, K=0 calcs
 */
      b = _mm256_unpacklo_pd(rB00, rB00);       /* rB00 rB00 rB00 rB00 */
      m    = _mm256_mul_pd(A, b);
      C20 = _mm256_add_pd(m, C20);  a    = _mm256_shuffle_pd(A, A, 0x5);
      b = _mm256_unpacklo_pd(rB01, rB01);
      m    = _mm256_mul_pd(A, b);
      C21 = _mm256_add_pd(m, C21);  C00 = _mm256_load_pd(pC0+8);
      b = _mm256_unpacklo_pd(rB02, rB02); 
      m    = _mm256_mul_pd(A, b);
      C22 = _mm256_add_pd(m, C22);  A = _mm256_load_pd(pA1+4);

      b = _mm256_unpacklo_pd(iB00, iB00);       /* iB00 iB00 iB00 iB00 */
      m    = _mm256_mul_pd(a, b);
      C20 = _mm256_addsub_pd(C20, m);  C01 = _mm256_load_pd(pC1+8);
      b = _mm256_unpacklo_pd(iB01, iB01);
      m    = _mm256_mul_pd(a, b);
      C21 = _mm256_addsub_pd(C21, m);  C02 = _mm256_load_pd(pC2+8);
      b = _mm256_unpacklo_pd(iB02, iB02);
      m    = _mm256_mul_pd(a, b);
      C22 = _mm256_addsub_pd(C22, m);   a    = _mm256_shuffle_pd(A, A, 0x5);
/*
 *    M=2, K=1 calcs
 */
      b = _mm256_unpackhi_pd(rB00, rB00);       /* rB10 rB10 rB10 rB10 */
      m    = _mm256_mul_pd(A, b);
      C20 = _mm256_add_pd(m, C20);
      b = _mm256_unpackhi_pd(rB01, rB01);
      m    = _mm256_mul_pd(A, b);
      C21 = _mm256_add_pd(m, C21);
      b = _mm256_unpackhi_pd(rB02, rB02);
      m    = _mm256_mul_pd(A, b);
      C22 = _mm256_add_pd(m, C22);   A = _mm256_load_pd(pA0+8);

      b = _mm256_unpackhi_pd(iB00, iB00);       /* iB10 iB10 iB10 iB10 */
      m    = _mm256_mul_pd(a, b);
      C20 = _mm256_addsub_pd(C20, m); _mm256_store_pd(pC0+4, C20);
      b = _mm256_unpackhi_pd(iB01, iB01);
      m    = _mm256_mul_pd(a, b);
      C21 = _mm256_addsub_pd(C21, m); _mm256_store_pd(pC1+4, C21);
      b = _mm256_unpackhi_pd(iB02, iB02);
      m    = _mm256_mul_pd(a, b);
      C22 = _mm256_addsub_pd(C22, m); _mm256_store_pd(pC2+4, C22);   
      a    = _mm256_shuffle_pd(A, A, 0x5);
   }
/*
 * Drain pipes
 */
   {
      register __m256d m, b;
/*
 *    Do M=K=0 calcs
 */
      b = _mm256_unpacklo_pd(rB00, rB00);       /* rB00 rB00 rB00 rB00 */
      m    = _mm256_mul_pd(A, b);
      C00 = _mm256_add_pd(m, C00);  
      b = _mm256_unpacklo_pd(rB01, rB01);       /* rB01 rB01 rB01 rB01 */
      m    = _mm256_mul_pd(A, b);
      C01 = _mm256_add_pd(m, C01);  
      b = _mm256_unpacklo_pd(rB02, rB02);       /* rB02 rB02 rB02 rB02 */
      m    = _mm256_mul_pd(A, b);
      C02 = _mm256_add_pd(m, C02);  A = _mm256_load_pd(pA1);

      b = _mm256_unpacklo_pd(iB00, iB00);       /* iB00 iB00 iB00 iB00 */
      m    = _mm256_mul_pd(a, b);
      C00 = _mm256_addsub_pd(C00, m);
      b = _mm256_unpacklo_pd(iB01, iB01);
      m    = _mm256_mul_pd(a, b);
      C01 = _mm256_addsub_pd(C01, m);
      b = _mm256_unpacklo_pd(iB02, iB02);
      m    = _mm256_mul_pd(a, b);
      C02 = _mm256_addsub_pd(C02, m); a    = _mm256_shuffle_pd(A, A, 0x5);
/*
 *    Do M=0, K=1 calcs
 */
      b = _mm256_unpackhi_pd(rB00, rB00);       /* rB10 rB10 rB10 rB10 */
      m    = _mm256_mul_pd(A, b);
      C00 = _mm256_add_pd(m, C00);
      b = _mm256_unpackhi_pd(rB01, rB01);       /* rB11 rB11 rB11 rB11 */
      m    = _mm256_mul_pd(A, b);
      C01 = _mm256_add_pd(m, C01);
      b = _mm256_unpackhi_pd(rB02, rB02);       /* rB12 rB12 rB12 rB12 */
      m    = _mm256_mul_pd(A, b);
      C02 = _mm256_add_pd(m, C02);

      b = _mm256_unpackhi_pd(iB00, iB00);       /* iB10 iB10 iB10 iB10 */
      m    = _mm256_mul_pd(a, b);
      C00 = _mm256_addsub_pd(C00, m); _mm256_store_pd(pC0, C00);
      b = _mm256_unpackhi_pd(iB01, iB01);  
      m    = _mm256_mul_pd(a, b);
      C01 = _mm256_addsub_pd(C01, m); _mm256_store_pd(pC1, C01);
      b = _mm256_unpackhi_pd(iB02, iB02); 
      m    = _mm256_mul_pd(a, b);
      C02 = _mm256_addsub_pd(C02, m); _mm256_store_pd(pC2, C02);
      if (!(M&2))
      {  
         A = _mm256_load_pd(pA0+4);
         C20 = _mm256_load_pd(pC0+4);
         C21 = _mm256_load_pd(pC1+4);
         C22 = _mm256_load_pd(pC2+4);
         a    = _mm256_shuffle_pd(A, A, 0x5);
/*
 *       Do M=2, K=0 calcs
 */
         b = _mm256_unpacklo_pd(rB00, rB00);       /* rB00 rB00 rB00 rB00 */
         m    = _mm256_mul_pd(A, b);
         C20 = _mm256_add_pd(m, C20);  a    = _mm256_shuffle_pd(A, A, 0x5);
         b = _mm256_unpacklo_pd(rB01, rB01);
         m    = _mm256_mul_pd(A, b);
         C21 = _mm256_add_pd(m, C21);
         b = _mm256_unpacklo_pd(rB02, rB02); 
         m    = _mm256_mul_pd(A, b);
         C22 = _mm256_add_pd(m, C22);  A = _mm256_load_pd(pA1+4);

         b = _mm256_unpacklo_pd(iB00, iB00);       /* iB00 iB00 iB00 iB00 */
         m    = _mm256_mul_pd(a, b);
         C20 = _mm256_addsub_pd(C20, m);
         b = _mm256_unpacklo_pd(iB01, iB01);
         m    = _mm256_mul_pd(a, b);
         C21 = _mm256_addsub_pd(C21, m);
         b = _mm256_unpacklo_pd(iB02, iB02);
         m    = _mm256_mul_pd(a, b);
         C22 = _mm256_addsub_pd(C22, m);   a    = _mm256_shuffle_pd(A, A, 0x5);
/*
 *       M=2, K=1 calcs
 */
         b = _mm256_unpackhi_pd(rB00, rB00);       /* rB10 rB10 rB10 rB10 */
         m    = _mm256_mul_pd(A, b);
         C20 = _mm256_add_pd(m, C20);
         b = _mm256_unpackhi_pd(rB01, rB01);
         m    = _mm256_mul_pd(A, b);
         C21 = _mm256_add_pd(m, C21);
         b = _mm256_unpackhi_pd(rB02, rB02);
         m    = _mm256_mul_pd(A, b);
         C22 = _mm256_add_pd(m, C22);  
   
         b = _mm256_unpackhi_pd(iB00, iB00);       /* iB10 iB10 iB10 iB10 */
         m    = _mm256_mul_pd(a, b);
         C20 = _mm256_addsub_pd(C20, m); _mm256_store_pd(pC0+4, C20);
         b = _mm256_unpackhi_pd(iB01, iB01);
         m    = _mm256_mul_pd(a, b);
         C21 = _mm256_addsub_pd(C21, m); _mm256_store_pd(pC1+4, C21);
         b = _mm256_unpackhi_pd(iB02, iB02);
         m    = _mm256_mul_pd(a, b);
         C22 = _mm256_addsub_pd(C22, m); _mm256_store_pd(pC2+4, C22);   
      }
   }
}
#else   /* 32 register version goes here */
   #define NRHS 3
/*
 * This routine optimized for OOE machines; for statically scheduled
 * machines, the asg of rCxx followed immediately by the dependent store will
 * be bad news.  Would need to pipeline another loop iteration to avoid.
 */
ATL_SINLINE void ATL_rk2(ATL_CINT M, const TYPE *pA0, ATL_CINT lda0, 
                         TYPE *B, ATL_CINT ldb0, TYPE *pC0, ATL_CINT ldc0) 
{
   ATL_CINT lda=lda0+lda0, ldb=ldb0+ldb0, ldc=ldc0+ldc0;
   ATL_CINT MM =  (M & 1) ? M-1 : M-2;
   ATL_INT i;
   TYPE *pC1=pC0+ldc, *pC2=pC0+(ldc<<1), *pC3=pC1+(ldc<<1);
   const TYPE *pA1=pA0+lda;
   const RTYPE rB00=(*B), iB00=B[1], rB01=B[ldb], iB01=B[ldb+1],
               rB02=B[ldb+ldb], iB02=B[ldb+ldb+1];
   const RTYPE rB10=B[2], iB10=B[3], rB11=B[ldb+2], iB11=B[ldb+3],
               rB12=B[ldb+ldb+2], iB12=B[ldb+ldb+3];
   RTYPE rC00, rC01, rC02, rC10, rC11, rC12;
   RTYPE iC00, iC01, iC02, iC10, iC11, iC12;
   RTYPE rA00, rA10, rA01, rA11, iA00, iA10, iA01, iA11;

/*
 * Fetch C and A for M=0
 */
   rC00 = *pC0; iC00 = pC0[1];
   rC01 = *pC1; iC01 = pC1[1];
   rC02 = *pC2; iC02 = pC2[1];
   rA00 = *pA0; iA00 = pA0[1];
   rA01 = *pA1; iA01 = pA1[1];
   for (i=0; i < MM; i += 2, pA0 += 4, pA1 += 4, pC0 += 4, pC1 += 4, pC2 += 4)
   {
/*
 *    M=0, K=0, fetch M=1 C's and then K=1's A's
 */
      rC00 -= rA00 * rB00; rC10 = pC0[2];
      iC00 -= rA00 * iB00; iC10 = pC0[3];
      rC01 -= rA00 * rB01; rC11 = pC1[2];
      iC01 -= rA00 * iB01; iC11 = pC1[3];
      rC02 -= rA00 * rB02; rC12 = pC2[2];
      iC02 -= rA00 * iB02; iC12 = pC2[3];

      rC00 += iA00 * iB00; rA10 = pA0[2];
      iC00 -= iA00 * rB00; iA10 = pA0[3];
      rC01 += iA00 * iB01; rA11 = pA1[2];
      iC01 -= iA00 * rB01; iA11 = pA1[3];
      rC02 += iA00 * iB02;
      iC02 -= iA00 * rB02;
/*
 *    K == 1, and then finished, so store C out
 */
      rC00 -= rA01 * rB10;
      iC00 -= rA01 * iB10;
      rC01 -= rA01 * rB11;
      iC01 -= rA01 * iB11;
      rC02 -= rA01 * rB12;
      iC02 -= rA01 * iB12;

      rC00 += iA01 * iB10; *pC0 = rC00;
      iC00 -= iA01 * rB10; pC0[1] = iC00;
      rC01 += iA01 * iB11; *pC1 = rC01;
      iC01 -= iA01 * rB11; pC1[1] = iC01;
      rC02 += iA01 * iB12; *pC2 = rC02;
      iC02 -= iA01 * rB12; pC2[1] = iC02;
/*
 *    M=1, K=0, fetch M=2's C's and A's
 */
      rC10 -= rA10 * rB00; rC00 = pC0[4];
      iC10 -= rA10 * iB00; iC00 = pC0[5];
      rC11 -= rA10 * rB01; rC01 = pC1[4];
      iC11 -= rA10 * iB01; iC01 = pC1[5];
      rC12 -= rA10 * rB02; rC02 = pC2[4];
      iC12 -= rA10 * iB02; iC02 = pC2[5];

      rC10 += iA10 * iB00; rA00 = pA0[4];
      iC10 -= iA10 * rB00; iA00 = pA0[5];
      rC11 += iA10 * iB01; rA01 = pA1[4];
      iC11 -= iA10 * rB01; iA01 = pA1[5];
      rC12 += iA10 * iB02;
      iC12 -= iA10 * rB02;
/*
 *    M=1, K=1
 */
      rC10 -= rA11 * rB10;
      iC10 -= rA11 * iB10;
      rC11 -= rA11 * rB11;
      iC11 -= rA11 * iB11;
      rC12 -= rA11 * rB12;
      iC12 -= rA11 * iB12;

      rC10 += iA11 * iB10; pC0[2] = rC10;
      iC10 -= iA11 * rB10; pC0[3] = iC10;
      rC11 += iA11 * iB11; pC1[2] = rC11;
      iC11 -= iA11 * rB11; pC1[3] = iC11;
      rC12 += iA11 * iB12; pC2[2] = rC12;
      iC12 -= iA11 * rB12; pC2[3] = iC12;
   }
/*
 * Drain pipeline
 */
/*
 *    M=0, K=0
 */
   rC00 -= rA00 * rB00; 
   iC00 -= rA00 * iB00; 
   rC01 -= rA00 * rB01; 
   iC01 -= rA00 * iB01; 
   rC02 -= rA00 * rB02; 
   iC02 -= rA00 * iB02; 

   rC00 += iA00 * iB00; 
   iC00 -= iA00 * rB00; 
   rC01 += iA00 * iB01; 
   iC01 -= iA00 * rB01; 
   rC02 += iA00 * iB02;
   iC02 -= iA00 * rB02;
/*
 * K == 1, and then finished, so store C out
 */
   rC00 -= rA01 * rB10;
   iC00 -= rA01 * iB10;
   rC01 -= rA01 * rB11;
   iC01 -= rA01 * iB11;
   rC02 -= rA01 * rB12;
   iC02 -= rA01 * iB12;

   rC00 += iA01 * iB10; *pC0 = rC00;
   iC00 -= iA01 * rB10; pC0[1] = iC00;
   rC01 += iA01 * iB11; *pC1 = rC01;
   iC01 -= iA01 * rB11; pC1[1] = iC01;
   rC02 += iA01 * iB12; *pC2 = rC02;
   iC02 -= iA01 * rB12; pC2[1] = iC02;
/*
 * M=1, K=0, fetch M=2's C's and A's
 */
   if (!(M&1))
   {
      rC10 = pC0[2];
      iC10 = pC0[3];
      rC11 = pC1[2];
      iC11 = pC1[3];
      rC12 = pC2[2];
      iC12 = pC2[3];
      rA10 = pA0[2];
      iA10 = pA0[3];
      rA11 = pA1[2];
      iA11 = pA1[3];
      rC10 -= rA10 * rB00;
      iC10 -= rA10 * iB00;
      rC11 -= rA10 * rB01;
      iC11 -= rA10 * iB01;
      rC12 -= rA10 * rB02;
      iC12 -= rA10 * iB02;

      rC10 += iA10 * iB00;
      iC10 -= iA10 * rB00;
      rC11 += iA10 * iB01;
      iC11 -= iA10 * rB01;
      rC12 += iA10 * iB02;
      iC12 -= iA10 * rB02;
/*
 *    M=1, K=1
 */
      rC10 -= rA11 * rB10;
      iC10 -= rA11 * iB10;
      rC11 -= rA11 * rB11;
      iC11 -= rA11 * iB11;
      rC12 -= rA11 * rB12;
      iC12 -= rA11 * iB12;

      rC10 += iA11 * iB10; pC0[2] = rC10;
      iC10 -= iA11 * rB10; pC0[3] = iC10;
      rC11 += iA11 * iB11; pC1[2] = rC11;
      iC11 -= iA11 * rB11; pC1[3] = iC11;
      rC12 += iA11 * iB12; pC2[2] = rC12;
      iC12 -= iA11 * rB12; pC2[3] = iC12;
   }
}
#endif

#if NRHS == 3
/* 
 * Solve 2x2 L with 3 RHS symbolically for complex arithmetic; 
 * Diagonal elements have already been inverted
 */
ATL_SINLINE void ATL_trsmU2(const TYPE *U, ATL_CINT ldu, TYPE *B, ATL_CINT ldb)
{
   ATL_CINT ldU=ldu+ldu, ldB = ldb+ldb, ldB2=(ldb<<2);
   const RTYPE rU00=(*U), iU00=U[1], rU01=U[ldU], iU01=U[ldU+1];
   const RTYPE rU11=U[ldU+2], iU11 = U[ldU+3];
   RTYPE rB00=(*B), iB00=B[1], rB10=B[2], iB10=B[3];
   RTYPE rB01=B[ldB], iB01=B[ldB+1], rB11=B[ldB+2], iB11=B[ldB+3];
   RTYPE rB02=B[ldB2], iB02=B[ldB2+1], rB12=B[ldB2+2], iB12=B[ldB2+3];
   RTYPE rX;
/*
 * x1 = b1 / U11;  U11 is recipricol, so solve x1 = b1 * U11;
 */
   rB10 = rB10 * rU11 - iB10 * iU11;
   rB11 = rB11 * rU11 - iB11 * iU11;
   rB12 = rB12 * rU11 - iB12 * iU11;
/*
 * x0 = (b0 - U01*x1) / U00, do x0 = (b0 - U01*x1) first
 */
   rB00 = rB00 - rU01*rB10 + iU01*iB10;
   iB00 = iB00 - rU01*iB10 - iU01*rB10;
   rB01 = rB01 - rU01*rB11 + iU01*iB11;
   iB01 = iB01 - rU01*iB11 - iU01*rB11;
   rB02 = rB02 - rU01*rB12 + iU01*iB12;
   iB02 = iB02 - rU01*iB12 - iU01*rB12;
/*
 * Finish x0 = (b0 - U01*x1) / U00 by multiplying by U00 (1/U00)
 */
   *B = rB00 * rU00 - iB00 * iU00;
   B[1] = rB00 * iU00 + iB00 * rU00;
   B[ldB]   = rB01 * rU00 - iB01 * iU00;
   B[ldB+1] = rB01 * iU00 + iB01 * rU00;
   B[ldB2]   = rB02 * rU00 - iB02 * iU00;
   B[ldB2+1] = rB02 * iU00 + iB02 * rU00;
}

/* 
 * Solve 2x2 L with 3 RHS symbolically for complex arithmetic; 
 * Diagonal elements have already been inverted
 */
ATL_SINLINE void ATL_trsmL2(const TYPE *L, ATL_CINT ldl, TYPE *B, ATL_CINT ldb0)
{
   const RTYPE rL00=(*L), iL00=L[1], rL10=L[2], iL10=L[3], 
               rL11=L[ldl+ldl+2], iL11=L[ldl+ldl+3];
   ATL_CINT ldb=ldb0+ldb0, ldb2=(ldb0<<2);
   RTYPE rB00=(*B), iB00=B[1], rB10=B[2], iB10=B[3];
   RTYPE rB01=B[ldb], iB01=B[ldb+1], rB11=B[ldb+2], iB11=B[ldb+3];
   RTYPE rB02=B[ldb2], iB02=B[ldb2+1], rB12=B[ldb2+2], iB12=B[ldb2+3];
   RTYPE rX;
/*
 * x0 = b0 / L00 
 */
   *B        = rX = rB00*rL00 - iB00*iL00;
   B[1]      = iB00 = rB00*iL00 + iB00*rL00; rB00 = rX;
   B[ldb]    = rX = rB01*rL00 - iB01*iL00;
   B[ldb+1]  = iB01 = rB01*iL00 + iB01*rL00; rB01 = rX;
   B[ldb2]   = rX = rB02*rL00 - iB02*iL00;
   B[ldb2+1] = iB02 = rB02*iL00 + iB02*rL00; rB02 = rX;
/*
 * x1 = (b1 - L10 * x0)  [divide by diagonal in next step]
 */
   rB10 = (rB10 - rL10*rB00 + iL10*iB00);
   iB10 = (iB10 - rL10*iB00 - iL10*rB00);
   rB11 = (rB11 - rL10*rB01 + iL10*iB01);
   iB11 = (iB11 - rL10*iB01 - iL10*rB01);
   rB12 = (rB12 - rL10*rB02 + iL10*iB02);
   iB12 = (iB12 - rL10*iB02 - iL10*rB02);
/*
 * Mult by recipricol of L11 to finish x1 = (b1 - L10 * x0)/L11.
 */
   B[2]      = rB10*rL11 - iB10*iL11;
   B[3]      = rB10*iL11 + iB10*rL11;
   B[ldb+2]  = rB11*rL11 - iB11*iL11;
   B[ldb+3]  = rB11*iL11 + iB11*rL11;
   B[ldb2+2] = rB12*rL11 - iB12*iL11;
   B[ldb2+3] = iB12 = rB12*iL11 + iB12*rL11;
}
#endif

@ROUT ATL_trsmKL_rk4 ATL_trsmKR_rk4
#if defined(ATL_AVX) && defined(DREAL)
   #define NRHS 3
   #define ATL_BINWRK 1
   #include <immintrin.h>
/*
 * Subtract off x0...x3 contribution to all remaining equations using a
 * rank-4 update with mu=4, nu=3, ku=4.  This version is for 16 AVX regs.
 * nu is the # of RHS, ku is the number of equations solved, and mu is 
 * unrolled only to enable software pipelinine of load/use.
 * Loop order is MKN, so that B is kept completely in registers, and
 * C and A are streamed in (and out, for C) from cache during the operation.
 */
ATL_SINLINE void ATL_rk4(ATL_CINT M, const TYPE *A, ATL_CINT lda, 
                           TYPE *pB0, ATL_CINT ldb, TYPE *C, ATL_CINT ldc) 
{ 
   const TYPE *pA0 = A, *pA1 = A+lda, 
              *pA2 = A+((lda)<<1), *pA3=pA1+((lda)<<1); 
   TYPE *pC0 = C, *pC1 = C+ldc, *pC2 = C+((ldc)<<1);
   ATL_CINT MM =  (M & 4) ? M-4 : M-8; 
   int i; 
   register __m256d rB00, rB01, rB02; 
   register __m256d rB20, rB21, rB22;
   register __m256d rC00, rC01, rC02;
   register __m256d rC40, rC41, rC42;
   register __m256d rA0, rA1;
 
   if (M < 4)
      return;
   rB00 = _mm256_broadcast_pd((void*)pB0);              /* B10 B00 B10 B00 */
   rB20 = _mm256_broadcast_pd((void*)(pB0+2));
   rB01 = _mm256_broadcast_pd((void*)(pB0+ldb));
   rB21 = _mm256_broadcast_pd((void*)(pB0+ldb+2));
   rB02 = _mm256_broadcast_pd((void*)(pB0+ldb+ldb));
   rB22 = _mm256_broadcast_pd((void*)(pB0+ldb+ldb+2));

   rC00 = _mm256_load_pd(pC0);                          /* C30 C20 C10 C00 */
   rC01 = _mm256_load_pd(pC1);
   rC02 = _mm256_load_pd(pC2);
   rA0  = _mm256_load_pd(pA0);                          /* A30 A20 A10, A00 */
   for (i=0; i < MM; i += 8, pA0 += 8, pA1 += 8, pA2 += 8, pA3 += 8,
        pC0 += 8, pC1 += 8, pC2 += 8)
   {
      register __m256d rB;

      rB = _mm256_unpacklo_pd(rB00, rB00);
      rB = _mm256_mul_pd(rB, rA0);
      rC00 = _mm256_sub_pd(rC00, rB); rA1 = _mm256_load_pd(pA1);
      rB = _mm256_unpacklo_pd(rB01, rB01);
      rB = _mm256_mul_pd(rB, rA0);
      rC01 = _mm256_sub_pd(rC01, rB); rC40 =_mm256_load_pd(pC0+4);
      rB = _mm256_unpacklo_pd(rB02, rB02);
      rB = _mm256_mul_pd(rB, rA0);
      rC02 = _mm256_sub_pd(rC02, rB); rA0 = _mm256_load_pd(pA2);

      rB = _mm256_unpackhi_pd(rB00, rB00);
      rB = _mm256_mul_pd(rB, rA1);
      rC00 = _mm256_sub_pd(rC00, rB); rC41 =_mm256_load_pd(pC1+4);
      rB = _mm256_unpackhi_pd(rB01, rB01);
      rB = _mm256_mul_pd(rB, rA1);
      rC01 = _mm256_sub_pd(rC01, rB); rC42 =_mm256_load_pd(pC2+4);
      rB = _mm256_unpackhi_pd(rB02, rB02);
      rB = _mm256_mul_pd(rB, rA1);
      rC02 = _mm256_sub_pd(rC02, rB); rA1 = _mm256_load_pd(pA3);

      rB = _mm256_unpacklo_pd(rB20, rB20);
      rB = _mm256_mul_pd(rB, rA0);
      rC00 = _mm256_sub_pd(rC00, rB);
      rB = _mm256_unpacklo_pd(rB21, rB21);
      rB = _mm256_mul_pd(rB, rA0);
      rC01 = _mm256_sub_pd(rC01, rB);
      rB = _mm256_unpacklo_pd(rB22, rB22);
      rB = _mm256_mul_pd(rB, rA0);
      rC02 = _mm256_sub_pd(rC02, rB); rA0 = _mm256_load_pd(pA0+4);

      rB = _mm256_unpackhi_pd(rB20, rB20);
      rB = _mm256_mul_pd(rB, rA1);
      rC00 = _mm256_sub_pd(rC00, rB); _mm256_store_pd(pC0, rC00);
      rB = _mm256_unpackhi_pd(rB21, rB21);
      rB = _mm256_mul_pd(rB, rA1);
      rC01 = _mm256_sub_pd(rC01, rB); _mm256_store_pd(pC1, rC01);
      rB = _mm256_unpackhi_pd(rB22, rB22);
      rB = _mm256_mul_pd(rB, rA1);
      rC02 = _mm256_sub_pd(rC02, rB); rA1 = _mm256_load_pd(pA1+4);
/*
 *    2nd row of C regs
 */
      rB = _mm256_unpacklo_pd(rB00, rB00);
      rB = _mm256_mul_pd(rB, rA0);
      rC40 = _mm256_sub_pd(rC40, rB); _mm256_store_pd(pC2, rC02);
      rB = _mm256_unpacklo_pd(rB01, rB01);
      rB = _mm256_mul_pd(rB, rA0);
      rC41 = _mm256_sub_pd(rC41, rB); rC00 = _mm256_load_pd(pC0+8);
      rB = _mm256_unpacklo_pd(rB02, rB02);
      rB = _mm256_mul_pd(rB, rA0);
      rC42 = _mm256_sub_pd(rC42, rB); rA0 = _mm256_load_pd(pA2+4);

      rB = _mm256_unpackhi_pd(rB00, rB00);
      rB = _mm256_mul_pd(rB, rA1);
      rC40 = _mm256_sub_pd(rC40, rB); rC01 = _mm256_load_pd(pC1+8);
      rB = _mm256_unpackhi_pd(rB01, rB01);
      rB = _mm256_mul_pd(rB, rA1);
      rC41 = _mm256_sub_pd(rC41, rB); rC02 = _mm256_load_pd(pC2+8);
      rB = _mm256_unpackhi_pd(rB02, rB02);
      rB = _mm256_mul_pd(rB, rA1);
      rC42 = _mm256_sub_pd(rC42, rB); rA1 = _mm256_load_pd(pA3+4);

      rB = _mm256_unpacklo_pd(rB20, rB20);
      rB = _mm256_mul_pd(rB, rA0);
      rC40 = _mm256_sub_pd(rC40, rB);
      rB = _mm256_unpacklo_pd(rB21, rB21);
      rB = _mm256_mul_pd(rB, rA0);
      rC41 = _mm256_sub_pd(rC41, rB);
      rB = _mm256_unpacklo_pd(rB22, rB22);
      rB = _mm256_mul_pd(rB, rA0);
      rC42 = _mm256_sub_pd(rC42, rB); rA0 = _mm256_load_pd(pA0+8);

      rB = _mm256_unpackhi_pd(rB20, rB20);
      rB = _mm256_mul_pd(rB, rA1);
      rC40 = _mm256_sub_pd(rC40, rB); _mm256_store_pd(pC0+4, rC40);
      rB = _mm256_unpackhi_pd(rB21, rB21);
      rB = _mm256_mul_pd(rB, rA1);
      rC41 = _mm256_sub_pd(rC41, rB); _mm256_store_pd(pC1+4, rC41);
      rB = _mm256_unpackhi_pd(rB22, rB22);
      rB = _mm256_mul_pd(rB, rA1);
      rC42 = _mm256_sub_pd(rC42, rB); _mm256_store_pd(pC2+4, rC42);
   }
/*
 * Drain C load/use pipeline
 */
   if (M-MM == 4)   /* drain pipe over 1 iteration */
   {
      register __m256d rB;

      rB = _mm256_unpacklo_pd(rB00, rB00);
      rB = _mm256_mul_pd(rB, rA0);
      rC00 = _mm256_sub_pd(rC00, rB); rA1 = _mm256_load_pd(pA1);
      rB = _mm256_unpacklo_pd(rB01, rB01);
      rB = _mm256_mul_pd(rB, rA0);
      rC01 = _mm256_sub_pd(rC01, rB);
      rB = _mm256_unpacklo_pd(rB02, rB02);
      rB = _mm256_mul_pd(rB, rA0);
      rC02 = _mm256_sub_pd(rC02, rB); rA0 = _mm256_load_pd(pA2);

      rB = _mm256_unpackhi_pd(rB00, rB00);
      rB = _mm256_mul_pd(rB, rA1);
      rC00 = _mm256_sub_pd(rC00, rB);
      rB = _mm256_unpackhi_pd(rB01, rB01);
      rB = _mm256_mul_pd(rB, rA1);
      rC01 = _mm256_sub_pd(rC01, rB);
      rB = _mm256_unpackhi_pd(rB02, rB02);
      rB = _mm256_mul_pd(rB, rA1);
      rC02 = _mm256_sub_pd(rC02, rB); rA1 = _mm256_load_pd(pA3);

      rB = _mm256_unpacklo_pd(rB20, rB20);
      rB = _mm256_mul_pd(rB, rA0);
      rC00 = _mm256_sub_pd(rC00, rB);
      rB = _mm256_unpacklo_pd(rB21, rB21);
      rB = _mm256_mul_pd(rB, rA0);
      rC01 = _mm256_sub_pd(rC01, rB);
      rB = _mm256_unpacklo_pd(rB22, rB22);
      rB = _mm256_mul_pd(rB, rA0);
      rC02 = _mm256_sub_pd(rC02, rB);

      rB = _mm256_unpackhi_pd(rB20, rB20);
      rB = _mm256_mul_pd(rB, rA1);
      rC00 = _mm256_sub_pd(rC00, rB); _mm256_store_pd(pC0, rC00);
      rB = _mm256_unpackhi_pd(rB21, rB21);
      rB = _mm256_mul_pd(rB, rA1);
      rC01 = _mm256_sub_pd(rC01, rB); _mm256_store_pd(pC1, rC01);
      rB = _mm256_unpackhi_pd(rB22, rB22);
      rB = _mm256_mul_pd(rB, rA1);
      rC02 = _mm256_sub_pd(rC02, rB); _mm256_store_pd(pC2, rC02);
   }
   else /* M-MM = 8, drain pipe over 2 iterations */
   {
      register __m256d rB;

      rB = _mm256_unpacklo_pd(rB00, rB00);
      rB = _mm256_mul_pd(rB, rA0);
      rC00 = _mm256_sub_pd(rC00, rB); rA1 = _mm256_load_pd(pA1);
      rB = _mm256_unpacklo_pd(rB01, rB01);
      rB = _mm256_mul_pd(rB, rA0);
      rC01 = _mm256_sub_pd(rC01, rB); rC40 =_mm256_load_pd(pC0+4);
      rB = _mm256_unpacklo_pd(rB02, rB02);
      rB = _mm256_mul_pd(rB, rA0);
      rC02 = _mm256_sub_pd(rC02, rB); rA0 = _mm256_load_pd(pA2);

      rB = _mm256_unpackhi_pd(rB00, rB00);
      rB = _mm256_mul_pd(rB, rA1);
      rC00 = _mm256_sub_pd(rC00, rB); rC41 =_mm256_load_pd(pC1+4);
      rB = _mm256_unpackhi_pd(rB01, rB01);
      rB = _mm256_mul_pd(rB, rA1);
      rC01 = _mm256_sub_pd(rC01, rB); rC42 =_mm256_load_pd(pC2+4);
      rB = _mm256_unpackhi_pd(rB02, rB02);
      rB = _mm256_mul_pd(rB, rA1);
      rC02 = _mm256_sub_pd(rC02, rB); rA1 = _mm256_load_pd(pA3);

      rB = _mm256_unpacklo_pd(rB20, rB20);
      rB = _mm256_mul_pd(rB, rA0);
      rC00 = _mm256_sub_pd(rC00, rB);
      rB = _mm256_unpacklo_pd(rB21, rB21);
      rB = _mm256_mul_pd(rB, rA0);
      rC01 = _mm256_sub_pd(rC01, rB);
      rB = _mm256_unpacklo_pd(rB22, rB22);
      rB = _mm256_mul_pd(rB, rA0);
      rC02 = _mm256_sub_pd(rC02, rB); rA0 = _mm256_load_pd(pA0+4);

      rB = _mm256_unpackhi_pd(rB20, rB20);
      rB = _mm256_mul_pd(rB, rA1);
      rC00 = _mm256_sub_pd(rC00, rB); _mm256_store_pd(pC0, rC00);
      rB = _mm256_unpackhi_pd(rB21, rB21);
      rB = _mm256_mul_pd(rB, rA1);
      rC01 = _mm256_sub_pd(rC01, rB); _mm256_store_pd(pC1, rC01);
      rB = _mm256_unpackhi_pd(rB22, rB22);
      rB = _mm256_mul_pd(rB, rA1);
      rC02 = _mm256_sub_pd(rC02, rB); rA1 = _mm256_load_pd(pA1+4);
/*
 *    2nd row of C regs
 */
      rB = _mm256_unpacklo_pd(rB00, rB00);
      rB = _mm256_mul_pd(rB, rA0);
      rC40 = _mm256_sub_pd(rC40, rB); _mm256_store_pd(pC2, rC02);
      rB = _mm256_unpacklo_pd(rB01, rB01);
      rB = _mm256_mul_pd(rB, rA0);
      rC41 = _mm256_sub_pd(rC41, rB);
      rB = _mm256_unpacklo_pd(rB02, rB02);
      rB = _mm256_mul_pd(rB, rA0);
      rC42 = _mm256_sub_pd(rC42, rB); rA0 = _mm256_load_pd(pA2+4);

      rB = _mm256_unpackhi_pd(rB00, rB00);
      rB = _mm256_mul_pd(rB, rA1);
      rC40 = _mm256_sub_pd(rC40, rB);
      rB = _mm256_unpackhi_pd(rB01, rB01);
      rB = _mm256_mul_pd(rB, rA1);
      rC41 = _mm256_sub_pd(rC41, rB);
      rB = _mm256_unpackhi_pd(rB02, rB02);
      rB = _mm256_mul_pd(rB, rA1);
      rC42 = _mm256_sub_pd(rC42, rB); rA1 = _mm256_load_pd(pA3+4);

      rB = _mm256_unpacklo_pd(rB20, rB20);
      rB = _mm256_mul_pd(rB, rA0);
      rC40 = _mm256_sub_pd(rC40, rB);
      rB = _mm256_unpacklo_pd(rB21, rB21);
      rB = _mm256_mul_pd(rB, rA0);
      rC41 = _mm256_sub_pd(rC41, rB);
      rB = _mm256_unpacklo_pd(rB22, rB22);
      rB = _mm256_mul_pd(rB, rA0);
      rC42 = _mm256_sub_pd(rC42, rB);

      rB = _mm256_unpackhi_pd(rB20, rB20);
      rB = _mm256_mul_pd(rB, rA1);
      rC40 = _mm256_sub_pd(rC40, rB); _mm256_store_pd(pC0+4, rC40);
      rB = _mm256_unpackhi_pd(rB21, rB21);
      rB = _mm256_mul_pd(rB, rA1);
      rC41 = _mm256_sub_pd(rC41, rB); _mm256_store_pd(pC1+4, rC41);
      rB = _mm256_unpackhi_pd(rB22, rB22);
      rB = _mm256_mul_pd(rB, rA1);
      rC42 = _mm256_sub_pd(rC42, rB); _mm256_store_pd(pC2+4, rC42);
   }
}
#elif defined(ATL_SSE2) && defined(DREAL)
   #define NRHS 3
   #define ATL_BINWRK 1
   #include <xmmintrin.h>
/*
 * Subtract off x0...x3 contribution to all remaining equations using a
 * rank-4 update with mu=4, nu=3, ku=4.  This version is for 16 SSE2 regs.
 * nu is the # of RHS, ku is the number of equations solved, and mu is 
 * unrolled only to enable software pipelinine of load/use.
 * Loop order is MKN, so that B is kept completely in registers, and
 * C and A are streamed in (and out, for C) from cache during the operation.
 */
ATL_SINLINE void ATL_rk4(ATL_CINT M, const TYPE *A, ATL_CINT lda, 
                           TYPE *pB0, ATL_CINT ldb, TYPE *C, ATL_CINT ldc) 
{ 
   const TYPE *pA0 = A, *pA1 = A+lda, 
              *pA2 = A+((lda)<<1), *pA3=pA1+((lda)<<1); 
   TYPE *pC0 = C, *pC1 = C+ldc, *pC2 = C+((ldc)<<1);
   const int MM = M-4; 
   int i; 
   register __m128d rB00, rB01, rB02; 
   register __m128d rB20, rB21, rB22;
   register __m128d rC00, rC01, rC02;
   register __m128d rC20, rC21, rC22;
   register __m128d rA0, rA1;
 
   if (M < 4)
      return;
   rB00 = _mm_load_pd(pB0);
   rB20 = _mm_load_pd(pB0+2);
   rB01 = _mm_load_pd(pB0+ldb);
   rB21 = _mm_load_pd(pB0+ldb+2);
   rB02 = _mm_load_pd(pB0+2*ldb);
   rB22 = _mm_load_pd(pB0+2*ldb+2);

   rC00 = _mm_load_pd(pC0);
   rC01 = _mm_load_pd(pC1);
   rC02 = _mm_load_pd(pC2);
   rA0  = _mm_load_pd(pA0);  /* A1, A0 */
   for (i=0; i < MM; i += 4, pA0 += 4, pA1 += 4, pA2 += 4, pA3 += 4,
        pC0 += 4, pC1 += 4, pC2 += 4)
   {
      register __m128d rB;

      rB = _mm_unpacklo_pd(rB00, rB00);
      rB = _mm_mul_pd(rB, rA0);
      rC00 = _mm_sub_pd(rC00, rB); rA1 = _mm_load_pd(pA1);
      rB = _mm_unpacklo_pd(rB01, rB01);
      rB = _mm_mul_pd(rB, rA0);
      rC01 = _mm_sub_pd(rC01, rB); rC20 =_mm_load_pd(pC0+2);
      rB = _mm_unpacklo_pd(rB02, rB02);
      rB = _mm_mul_pd(rB, rA0);
      rC02 = _mm_sub_pd(rC02, rB); rA0 = _mm_load_pd(pA2);

      rB = _mm_unpackhi_pd(rB00, rB00);
      rB = _mm_mul_pd(rB, rA1);
      rC00 = _mm_sub_pd(rC00, rB); rC21 =_mm_load_pd(pC1+2);
      rB = _mm_unpackhi_pd(rB01, rB01);
      rB = _mm_mul_pd(rB, rA1);
      rC01 = _mm_sub_pd(rC01, rB); rC22 =_mm_load_pd(pC2+2);
      rB = _mm_unpackhi_pd(rB02, rB02);
      rB = _mm_mul_pd(rB, rA1);
      rC02 = _mm_sub_pd(rC02, rB); rA1 = _mm_load_pd(pA3);

      rB = _mm_unpacklo_pd(rB20, rB20);
      rB = _mm_mul_pd(rB, rA0);
      rC00 = _mm_sub_pd(rC00, rB);
      rB = _mm_unpacklo_pd(rB21, rB21);
      rB = _mm_mul_pd(rB, rA0);
      rC01 = _mm_sub_pd(rC01, rB);
      rB = _mm_unpacklo_pd(rB22, rB22);
      rB = _mm_mul_pd(rB, rA0);
      rC02 = _mm_sub_pd(rC02, rB); rA0 = _mm_load_pd(pA0+2);

      rB = _mm_unpackhi_pd(rB20, rB20);
      rB = _mm_mul_pd(rB, rA1);
      rC00 = _mm_sub_pd(rC00, rB); _mm_store_pd(pC0, rC00);
      rB = _mm_unpackhi_pd(rB21, rB21);
      rB = _mm_mul_pd(rB, rA1);
      rC01 = _mm_sub_pd(rC01, rB); _mm_store_pd(pC1, rC01);
      rB = _mm_unpackhi_pd(rB22, rB22);
      rB = _mm_mul_pd(rB, rA1);
      rC02 = _mm_sub_pd(rC02, rB); rA1 = _mm_load_pd(pA1+2);
/*
 *    2nd row of C regs
 */
      rB = _mm_unpacklo_pd(rB00, rB00);
      rB = _mm_mul_pd(rB, rA0);
      rC20 = _mm_sub_pd(rC20, rB); _mm_store_pd(pC2, rC02);
      rB = _mm_unpacklo_pd(rB01, rB01);
      rB = _mm_mul_pd(rB, rA0);
      rC21 = _mm_sub_pd(rC21, rB); rC00 = _mm_load_pd(pC0+4);
      rB = _mm_unpacklo_pd(rB02, rB02);
      rB = _mm_mul_pd(rB, rA0);
      rC22 = _mm_sub_pd(rC22, rB); rA0 = _mm_load_pd(pA2+2);

      rB = _mm_unpackhi_pd(rB00, rB00);
      rB = _mm_mul_pd(rB, rA1);
      rC20 = _mm_sub_pd(rC20, rB); rC01 = _mm_load_pd(pC1+4);
      rB = _mm_unpackhi_pd(rB01, rB01);
      rB = _mm_mul_pd(rB, rA1);
      rC21 = _mm_sub_pd(rC21, rB); rC02 = _mm_load_pd(pC2+4);
      rB = _mm_unpackhi_pd(rB02, rB02);
      rB = _mm_mul_pd(rB, rA1);
      rC22 = _mm_sub_pd(rC22, rB); rA1 = _mm_load_pd(pA3+2);

      rB = _mm_unpacklo_pd(rB20, rB20);
      rB = _mm_mul_pd(rB, rA0);
      rC20 = _mm_sub_pd(rC20, rB);
      rB = _mm_unpacklo_pd(rB21, rB21);
      rB = _mm_mul_pd(rB, rA0);
      rC21 = _mm_sub_pd(rC21, rB);
      rB = _mm_unpacklo_pd(rB22, rB22);
      rB = _mm_mul_pd(rB, rA0);
      rC22 = _mm_sub_pd(rC22, rB); rA0 = _mm_load_pd(pA0+4);

      rB = _mm_unpackhi_pd(rB20, rB20);
      rB = _mm_mul_pd(rB, rA1);
      rC20 = _mm_sub_pd(rC20, rB); _mm_store_pd(pC0+2, rC20);
      rB = _mm_unpackhi_pd(rB21, rB21);
      rB = _mm_mul_pd(rB, rA1);
      rC21 = _mm_sub_pd(rC21, rB); _mm_store_pd(pC1+2, rC21);
      rB = _mm_unpackhi_pd(rB22, rB22);
      rB = _mm_mul_pd(rB, rA1);
      rC22 = _mm_sub_pd(rC22, rB); _mm_store_pd(pC2+2, rC22);
   }
/*
 * Drain C load/use pipeline
 */
   {
      register __m128d rB;

      rB = _mm_unpacklo_pd(rB00, rB00);
      rB = _mm_mul_pd(rB, rA0);
      rC00 = _mm_sub_pd(rC00, rB); rA1 = _mm_load_pd(pA1);
      rB = _mm_unpacklo_pd(rB01, rB01);
      rB = _mm_mul_pd(rB, rA0);
      rC01 = _mm_sub_pd(rC01, rB); rC20 =_mm_load_pd(pC0+2);
      rB = _mm_unpacklo_pd(rB02, rB02);
      rB = _mm_mul_pd(rB, rA0);
      rC02 = _mm_sub_pd(rC02, rB); rA0 = _mm_load_pd(pA2);

      rB = _mm_unpackhi_pd(rB00, rB00);
      rB = _mm_mul_pd(rB, rA1);
      rC00 = _mm_sub_pd(rC00, rB); rC21 =_mm_load_pd(pC1+2);
      rB = _mm_unpackhi_pd(rB01, rB01);
      rB = _mm_mul_pd(rB, rA1);
      rC01 = _mm_sub_pd(rC01, rB); rC22 =_mm_load_pd(pC2+2);
      rB = _mm_unpackhi_pd(rB02, rB02);
      rB = _mm_mul_pd(rB, rA1);
      rC02 = _mm_sub_pd(rC02, rB); rA1 = _mm_load_pd(pA3);

      rB = _mm_unpacklo_pd(rB20, rB20);
      rB = _mm_mul_pd(rB, rA0);
      rC00 = _mm_sub_pd(rC00, rB);
      rB = _mm_unpacklo_pd(rB21, rB21);
      rB = _mm_mul_pd(rB, rA0);
      rC01 = _mm_sub_pd(rC01, rB);
      rB = _mm_unpacklo_pd(rB22, rB22);
      rB = _mm_mul_pd(rB, rA0);
      rC02 = _mm_sub_pd(rC02, rB); rA0 = _mm_load_pd(pA0+2);

      rB = _mm_unpackhi_pd(rB20, rB20);
      rB = _mm_mul_pd(rB, rA1);
      rC00 = _mm_sub_pd(rC00, rB); _mm_store_pd(pC0, rC00);
      rB = _mm_unpackhi_pd(rB21, rB21);
      rB = _mm_mul_pd(rB, rA1);
      rC01 = _mm_sub_pd(rC01, rB); _mm_store_pd(pC1, rC01);
      rB = _mm_unpackhi_pd(rB22, rB22);
      rB = _mm_mul_pd(rB, rA1);
      rC02 = _mm_sub_pd(rC02, rB); rA1 = _mm_load_pd(pA1+2);
/*
 *    2nd row of C regs
 */
      rB = _mm_unpacklo_pd(rB00, rB00);
      rB = _mm_mul_pd(rB, rA0);
      rC20 = _mm_sub_pd(rC20, rB); _mm_store_pd(pC2, rC02);
      rB = _mm_unpacklo_pd(rB01, rB01);
      rB = _mm_mul_pd(rB, rA0);
      rC21 = _mm_sub_pd(rC21, rB);
      rB = _mm_unpacklo_pd(rB02, rB02);
      rB = _mm_mul_pd(rB, rA0);
      rC22 = _mm_sub_pd(rC22, rB); rA0 = _mm_load_pd(pA2+2);

      rB = _mm_unpackhi_pd(rB00, rB00);
      rB = _mm_mul_pd(rB, rA1);
      rC20 = _mm_sub_pd(rC20, rB);
      rB = _mm_unpackhi_pd(rB01, rB01);
      rB = _mm_mul_pd(rB, rA1);
      rC21 = _mm_sub_pd(rC21, rB);
      rB = _mm_unpackhi_pd(rB02, rB02);
      rB = _mm_mul_pd(rB, rA1);
      rC22 = _mm_sub_pd(rC22, rB); rA1 = _mm_load_pd(pA3+2);

      rB = _mm_unpacklo_pd(rB20, rB20);
      rB = _mm_mul_pd(rB, rA0);
      rC20 = _mm_sub_pd(rC20, rB);
      rB = _mm_unpacklo_pd(rB21, rB21);
      rB = _mm_mul_pd(rB, rA0);
      rC21 = _mm_sub_pd(rC21, rB);
      rB = _mm_unpacklo_pd(rB22, rB22);
      rB = _mm_mul_pd(rB, rA0);
      rC22 = _mm_sub_pd(rC22, rB);

      rB = _mm_unpackhi_pd(rB20, rB20);
      rB = _mm_mul_pd(rB, rA1);
      rC20 = _mm_sub_pd(rC20, rB); _mm_store_pd(pC0+2, rC20);
      rB = _mm_unpackhi_pd(rB21, rB21);
      rB = _mm_mul_pd(rB, rA1);
      rC21 = _mm_sub_pd(rC21, rB); _mm_store_pd(pC1+2, rC21);
      rB = _mm_unpackhi_pd(rB22, rB22);
      rB = _mm_mul_pd(rB, rA1);
      rC22 = _mm_sub_pd(rC22, rB); _mm_store_pd(pC2+2, rC22);
   }
}
#elif defined(ATL_SSE2) && defined(SREAL)
   #define NRHS 4
   #define ATL_BINWRK 1
   #include <xmmintrin.h>
/*
 * Subtract off x0...x3 contribution to all remaining equations using a
 * rank-4 update with mu=8, nu=4, ku=4.  This version is for 16 SSE regs.
 * nu is the # of RHS, ku is the number of equations solved, and mu is 
 * unrolled only to enable vectorizations & software pipelinine of load/use.
 * Code operates on any multiple of 4 despite using MU=8.
 * Loop order is MKN, so that B is kept completely in registers, and
 * C and A are streamed in (and out, for C) from cache during the operation.
 */
ATL_SINLINE void ATL_rk4(ATL_CINT M, const TYPE *A, ATL_CINT lda, 
                           TYPE *pB0, ATL_CINT ldb, TYPE *C, ATL_CINT ldc) 
{ 
   const TYPE *pA0 = A, *pA1 = A+lda, 
              *pA2 = A+((lda)<<1), *pA3=pA1+((lda)<<1); 
   TYPE *pC0 = C, *pC1 = C+ldc, *pC2 = C+((ldc)<<1), *pC3 = pC2+ldc;
   ATL_CINT MM =  (M & 4) ? M-4 : M-8; 
   int i; 
   register __m128 rB00, rB01, rB02, rB03; 
   register __m128 rC00, rC01, rC02, rC03;
   register __m128 rC40, rC41, rC42, rC43;
   register __m128 rA0, rA1;
 
   if (M < 4)
      return;
   rB00 = _mm_load_ps(pB0);
   rB01 = _mm_load_ps(pB0+ldb);
   rB02 = _mm_load_ps(pB0+(ldb<<1));
   rB03 = _mm_load_ps(pB0+(ldb<<1)+ldb);

   rC00 = _mm_load_ps(pC0);
   rC01 = _mm_load_ps(pC1);
   rC02 = _mm_load_ps(pC2);
   rC03 = _mm_load_ps(pC3);

   rA0 = _mm_load_ps(pA0);

   for (i=0; i < MM; i += 8, pA0 += 8, pA1 += 8, pA2 += 8, pA3 += 8,
        pC0 += 8, pC1 += 8, pC2 += 8, pC3 += 8)
   {
      register __m128 rB;
/*
 *    K=0 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC00 = _mm_sub_ps(rC00, rB);  rA1 = _mm_load_ps(pA1);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC01 = _mm_sub_ps(rC01, rB);  rC40 = _mm_load_ps(pC0+4);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC02 = _mm_sub_ps(rC02, rB);  rC41 = _mm_load_ps(pC1+4);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC03 = _mm_sub_ps(rC03, rB);  rC42 = _mm_load_ps(pC2+4);
/*
 *    K=1 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC00 = _mm_sub_ps(rC00, rB);  rA0 = _mm_load_ps(pA2);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC01 = _mm_sub_ps(rC01, rB);  rC43 = _mm_load_ps(pC3+4);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC02 = _mm_sub_ps(rC02, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC03 = _mm_sub_ps(rC03, rB);  rA1 = _mm_load_ps(pA3);
/*
 *    K=2 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC00 = _mm_sub_ps(rC00, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC01 = _mm_sub_ps(rC01, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC02 = _mm_sub_ps(rC02, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC03 = _mm_sub_ps(rC03, rB);  rA0 = _mm_load_ps(pA0+4);
/*
 *    K=3 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC00 = _mm_sub_ps(rC00, rB);  _mm_store_ps(pC0, rC00);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC01 = _mm_sub_ps(rC01, rB);  _mm_store_ps(pC1, rC01);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC02 = _mm_sub_ps(rC02, rB);  _mm_store_ps(pC2, rC02);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC03 = _mm_sub_ps(rC03, rB); _mm_store_ps(pC3, rC03);
      
/*
 *    K=0 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC40 = _mm_sub_ps(rC40, rB);  rA1 = _mm_load_ps(pA1+4);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC41 = _mm_sub_ps(rC41, rB);  rC00 = _mm_load_ps(pC0+8);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC42 = _mm_sub_ps(rC42, rB);  rC01 = _mm_load_ps(pC1+8);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC43 = _mm_sub_ps(rC43, rB);  rC02 = _mm_load_ps(pC2+8);
/*
 *    K=1 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC40 = _mm_sub_ps(rC40, rB);  rA0 = _mm_load_ps(pA2+4);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC41 = _mm_sub_ps(rC41, rB);  rC03 = _mm_load_ps(pC3+8);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC42 = _mm_sub_ps(rC42, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC43 = _mm_sub_ps(rC43, rB);  rA1 = _mm_load_ps(pA3+4);
/*
 *    K=2 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC40 = _mm_sub_ps(rC40, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC41 = _mm_sub_ps(rC41, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC42 = _mm_sub_ps(rC42, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC43 = _mm_sub_ps(rC43, rB);  rA0 = _mm_load_ps(pA0+8);
/*
 *    K=3 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC40 = _mm_sub_ps(rC40, rB);  _mm_store_ps(pC0+4, rC40);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC41 = _mm_sub_ps(rC41, rB);  _mm_store_ps(pC1+4, rC41);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC42 = _mm_sub_ps(rC42, rB);  _mm_store_ps(pC2+4, rC42);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC43 = _mm_sub_ps(rC43, rB); _mm_store_ps(pC3+4, rC43);
   }
/* 
 * If orig M was multiple of 4 rather than 8, drain pipe over last 4 rows
 */
   if (M&4)
   {
      register __m128 rB;
/*
 *    K=0 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC00 = _mm_sub_ps(rC00, rB);  rA1 = _mm_load_ps(pA1);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC01 = _mm_sub_ps(rC01, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC02 = _mm_sub_ps(rC02, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC03 = _mm_sub_ps(rC03, rB);  rA0 = _mm_load_ps(pA2);
/*
 *    K=1 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC00 = _mm_sub_ps(rC00, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC01 = _mm_sub_ps(rC01, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC02 = _mm_sub_ps(rC02, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC03 = _mm_sub_ps(rC03, rB);  rA1 = _mm_load_ps(pA3);
/*
 *    K=2 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC00 = _mm_sub_ps(rC00, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC01 = _mm_sub_ps(rC01, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC02 = _mm_sub_ps(rC02, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC03 = _mm_sub_ps(rC03, rB);
/*
 *    K=3 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC00 = _mm_sub_ps(rC00, rB);  _mm_store_ps(pC0, rC00);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC01 = _mm_sub_ps(rC01, rB);  _mm_store_ps(pC1, rC01);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC02 = _mm_sub_ps(rC02, rB);  _mm_store_ps(pC2, rC02);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC03 = _mm_sub_ps(rC03, rB); _mm_store_ps(pC3, rC03);
   }
   else /* drain pipe with MU=8 */
   {
      register __m128 rB;
/*
 *    K=0 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC00 = _mm_sub_ps(rC00, rB);  rA1 = _mm_load_ps(pA1);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC01 = _mm_sub_ps(rC01, rB);  rC40 = _mm_load_ps(pC0+4);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC02 = _mm_sub_ps(rC02, rB);  rC41 = _mm_load_ps(pC1+4);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC03 = _mm_sub_ps(rC03, rB);  rC42 = _mm_load_ps(pC2+4);
/*
 *    K=1 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC00 = _mm_sub_ps(rC00, rB);  rA0 = _mm_load_ps(pA2);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC01 = _mm_sub_ps(rC01, rB);  rC43 = _mm_load_ps(pC3+4);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC02 = _mm_sub_ps(rC02, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC03 = _mm_sub_ps(rC03, rB);  rA1 = _mm_load_ps(pA3);
/*
 *    K=2 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC00 = _mm_sub_ps(rC00, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC01 = _mm_sub_ps(rC01, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC02 = _mm_sub_ps(rC02, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC03 = _mm_sub_ps(rC03, rB);  rA0 = _mm_load_ps(pA0+4);
/*
 *    K=3 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC00 = _mm_sub_ps(rC00, rB);  _mm_store_ps(pC0, rC00);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC01 = _mm_sub_ps(rC01, rB);  _mm_store_ps(pC1, rC01);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC02 = _mm_sub_ps(rC02, rB);  _mm_store_ps(pC2, rC02);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC03 = _mm_sub_ps(rC03, rB); _mm_store_ps(pC3, rC03);
      
/*
 *    K=0 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC40 = _mm_sub_ps(rC40, rB);  rA1 = _mm_load_ps(pA1+4);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC41 = _mm_sub_ps(rC41, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC42 = _mm_sub_ps(rC42, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC43 = _mm_sub_ps(rC43, rB);
/*
 *    K=1 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC40 = _mm_sub_ps(rC40, rB);  rA0 = _mm_load_ps(pA2+4);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC41 = _mm_sub_ps(rC41, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC42 = _mm_sub_ps(rC42, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC43 = _mm_sub_ps(rC43, rB);  rA1 = _mm_load_ps(pA3+4);
/*
 *    K=2 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC40 = _mm_sub_ps(rC40, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC41 = _mm_sub_ps(rC41, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC42 = _mm_sub_ps(rC42, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC43 = _mm_sub_ps(rC43, rB);
/*
 *    K=3 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC40 = _mm_sub_ps(rC40, rB);  _mm_store_ps(pC0+4, rC40);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC41 = _mm_sub_ps(rC41, rB);  _mm_store_ps(pC1+4, rC41);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC42 = _mm_sub_ps(rC42, rB);  _mm_store_ps(pC2+4, rC42);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC43 = _mm_sub_ps(rC43, rB); _mm_store_ps(pC3+4, rC43);
   }
}
#else
   #define NRHS 4
   #define ATL_BINWRK 0
/*
 * Subtract off x0...x3 contribution to all remaining equations using a
 * rank-4 update with mu=2, nu=4, ku=4.  This version is for 32 scalar
 * registers, and assumes the scalar registers rB00..rB33 are live on entry.
 * nu is the # of RHS, ku is the number of equations solved, and mu is 
 * unrolled only to enable software pipelinine of load/use.
 * Loop order is MKN, so that B is kept completely in registers, and
 * C and A are streamed in (and out, for C) from cache during the operation.
 */
#define ATL_rk4(M_, A_, lda_, C_, ldc_) if (M_ > 1) \
{ \
   const TYPE *pA0 = A_, *pA1 = A_+lda_, \
              *pA2 = A_+((lda_)<<1), *pA3=pA1+((lda_)<<1); \
   TYPE *pC0 = C_, *pC1 = C_+ldc_, \
               *pC2 = C_+((ldc_)<<1), *pC3=pC1+((ldc_)<<1); \
   register TYPE rC00= *pC0, rC01= *pC1, rC02 = *pC2, rC03 = *pC3; \
   register TYPE rc00, rc01, rc02, rc03; \
   register TYPE rA0 = *pA0, rA1; \
   ATL_CINT MM = M_ - 2; \
   ATL_INT i; \
 \
   for (i=0; i < MM; i += 2, pA0 += 2, pA1 += 2, pA2 += 2, pA3 += 2, \
        pC0 += 2, pC1 += 2, pC2 += 2, pC3 += 2) \
   { \
      rC00 -= rA0 * rB00; rA1 = *pA1; \
      rC01 -= rA0 * rB01; rc00 = pC0[1]; \
      rC02 -= rA0 * rB02; rc01 = pC1[1]; \
      rC03 -= rA0 * rB03; rc02 = pC2[1]; \
 \
      rC00 -= rA1 * rB10; rA0 = *pA2; \
      rC01 -= rA1 * rB11; rc03 = pC3[1]; \
      rC02 -= rA1 * rB12;  \
      rC03 -= rA1 * rB13;  \
       \
      rC00 -= rA0 * rB20; rA1 = *pA3; \
      rC01 -= rA0 * rB21; \
      rC02 -= rA0 * rB22;  \
      rC03 -= rA0 * rB23; rA0 = pA0[1]; \
       \
      rC00 -= rA1 * rB30; *pC0 = rC00; \
      rC01 -= rA1 * rB31; *pC1 = rC01; \
      rC02 -= rA1 * rB32; *pC2 = rC02; \
      rC03 -= rA1 * rB33; *pC3 = rC03; \
       \
      rc00 -= rA0 * rB00; rA1 = pA1[1]; \
      rc01 -= rA0 * rB01; rC00 = pC0[2]; \
      rc02 -= rA0 * rB02; rC01 = pC1[2]; \
      rc03 -= rA0 * rB03; rC02 = pC2[2]; \
       \
      rc00 -= rA1 * rB10; rA0 = pA2[1]; \
      rc01 -= rA1 * rB11; rC03 = pC3[2]; \
      rc02 -= rA1 * rB12; \
      rc03 -= rA1 * rB13;  \
       \
      rc00 -= rA0 * rB20; rA1 = pA3[1]; \
      rc01 -= rA0 * rB21; \
      rc02 -= rA0 * rB22;  \
      rc03 -= rA0 * rB23; rA0 = pA0[2]; \
       \
      rc00 -= rA1 * rB30; pC0[1] = rc00; \
      rc01 -= rA1 * rB31; pC1[1] = rc01; \
      rc02 -= rA1 * rB32; pC2[1] = rc02; \
      rc03 -= rA1 * rB33; pC3[1] = rc03; \
   } \
/* \
 *  Drain the C fetch/store pipe \
 */ \
   rC00 -= rA0 * rB00; rA1 = *pA1; \
   rC01 -= rA0 * rB01; rc00 = pC0[1]; \
   rC02 -= rA0 * rB02; rc01 = pC1[1]; \
   rC03 -= rA0 * rB03; rc02 = pC2[1]; \
 \
   rC00 -= rA1 * rB10; rA0 = *pA2; \
   rC01 -= rA1 * rB11; rc03 = pC3[1]; \
   rC02 -= rA1 * rB12;  \
   rC03 -= rA1 * rB13;  \
    \
   rC00 -= rA0 * rB20; rA1 = *pA3; \
   rC01 -= rA0 * rB21; \
   rC02 -= rA0 * rB22;  \
   rC03 -= rA0 * rB23; rA0 = pA0[1]; \
    \
   rC00 -= rA1 * rB30; *pC0 = rC00; \
   rC01 -= rA1 * rB31; *pC1 = rC01; \
   rC02 -= rA1 * rB32; *pC2 = rC02; \
   rC03 -= rA1 * rB33; *pC3 = rC03; \
    \
   rc00 -= rA0 * rB00; rA1 = pA1[1]; \
   rc01 -= rA0 * rB01; \
   rc02 -= rA0 * rB02; \
   rc03 -= rA0 * rB03; \
    \
   rc00 -= rA1 * rB10; rA0 = pA2[1]; \
   rc01 -= rA1 * rB11; \
   rc02 -= rA1 * rB12; \
   rc03 -= rA1 * rB13; \
    \
   rc00 -= rA0 * rB20; rA1 = pA3[1]; \
   rc01 -= rA0 * rB21; \
   rc02 -= rA0 * rB22; \
   rc03 -= rA0 * rB23; \
    \
   rc00 -= rA1 * rB30; pC0[1] = rc00; \
   rc01 -= rA1 * rB31; pC1[1] = rc01; \
   rc02 -= rA1 * rB32; pC2[1] = rc02; \
   rc03 -= rA1 * rB33; pC3[1] = rc03; \
}
#endif

#if NRHS == 3
/*
 * Solve 4x4 L with 3 RHS symbolically
 * Answer is output into rBxx regs, which are live on input and output
 */
#define  ATL_trsmL4(L_, ldl_, r_, ldr_) \
{ \
   const RTYPE L00=(*(L_)), L10=L_[1], L20=L_[2], L30=L_[3]; \
   const RTYPE L11=L_[ldl_+1], L21=L_[ldl_+2], L31=a[ldl_+3]; \
   const RTYPE L22=L_[2*(ldl_)+2], L32=L_[2*(ldl_)+3]; \
   const RTYPE L33=L_[3*(ldl_)+3]; \
/* \
 * x0 = b0 / L00 \
 */ \
   rB00 *= L00; \
   rB01 *= L00; \
   rB02 *= L00; \
/* \
 * x1 = (b1 - L10 * x0) / L11 \
 */ \
   rB10 = (rB10 - L10*rB00) * L11;  \
   rB11 = (rB11 - L10*rB01) * L11; \
   rB12 = (rB12 - L10*rB02) * L11; \
   ATL_pfl1W(r_ + ((ldr_)<<2)); \
/* \
 * x2 = (b2 - L20*x0 - L21*x1) / L22 \
 */ \
   rB20 = (rB20 - L20*rB00 - L21*rB10) * L22; \
   rB21 = (rB21 - L20*rB01 - L21*rB11) * L22; \
   rB22 = (rB22 - L20*rB02 - L21*rB12) * L22; \
   ATL_pfl1W(r_ + ldr_+((ldr_)<<2)); \
/* \
 * x3 = (b3 - L30*x0 - L31*x1 - L32*x2) / L33 \
 */ \
   rB30 = (rB30 - L30*rB00 - L31*rB10 - L32*rB20) * L33; \
   rB31 = (rB31 - L30*rB01 - L31*rB11 - L32*rB21) * L33; \
   rB32 = (rB32 - L30*rB02 - L31*rB12 - L32*rB22) * L33; \
   ATL_pfl1W(r_ + ((ldr_)<<1)+((ldr_)<<2)); \
}  /* complete 4x4 NRHS=3 solve block */

#define  ATL_trsmU4(U_, ldu_, r_, ldr_) \
{ \
   const RTYPE U00=(*(U_)); \
   const RTYPE U01=(U_)[ldu_], U11=(U_)[ldu_+1]; \
   const RTYPE U02=(U_)[2*(ldu_)], U12= *(U_+2*(ldu_)+1),  \
               U22 = *(U_+2*(ldu_)+2); \
   const RTYPE U03 = *(U_+3*(ldu_)), U13 = *(U_+3*(ldu_)+1), \
               U23 = *(U_+3*(ldu_)+2), U33 = *(U_+3*(ldu_)+3); \
\
/* \
 * x3 = b3 / U33 \
 */ \
   rB30 *= U33; \
   rB31 *= U33; \
   rB32 *= U33; \
   ATL_pfl1W(r_ + ((ldr_)<<2)); \
/* \
 * x2 = (b2 - U23 * x3) / U22 \
 */ \
   rB20 = (rB20 - U23*rB30) * U22;  \
   rB21 = (rB21 - U23*rB31) * U22; \
   rB22 = (rB22 - U23*rB32) * U22; \
   ATL_pfl1W(r_ + ldr_+((ldr_)<<2)); \
/* \
 * x1 = (b1 - U12*x2 - U13*x3) / U11 \
 */ \
   rB10 = (rB10 - U12*rB20 - U13*rB30) * U11; \
   rB11 = (rB11 - U12*rB21 - U13*rB31) * U11; \
   rB12 = (rB12 - U12*rB22 - U13*rB32) * U11; \
   ATL_pfl1W(r_ + ((ldr_)<<1)+((ldr_)<<2)); \
/* \
 * x0 = (b0 - U01*x1 - U02*x2 - U03*x3) / U00 \
 */ \
   rB00 = (rB00 - U01*rB10 - U02*rB20 - U03*rB30) * U00; \
   rB01 = (rB01 - U01*rB11 - U02*rB21 - U03*rB31) * U00; \
   ATL_pfl1W(r_ + ldr_+((ldr_)<<1)+((ldr_)<<2)); \
   rB02 = (rB02 - U01*rB12 - U02*rB22 - U03*rB32) * U00; \
}  /* complete M=4, N=3 solve block */
#elif NRHS == 4
/*
 * Solve 4x4 L with 4 RHS symbolically
 * Answer is output into rBxx regs, which are live on input and output
 */
#define  ATL_trsmL4(L_, ldl_, r_, ldr_) \
{ \
   const RTYPE L00=(*(L_)), L10=L_[1], L20=L_[2], L30=L_[3]; \
   const RTYPE L11=L_[ldl_+1], L21=L_[ldl_+2], L31=a[ldl_+3]; \
   const RTYPE L22=L_[2*(ldl_)+2], L32=L_[2*(ldl_)+3]; \
   const RTYPE L33=L_[3*(ldl_)+3]; \
/* \
 * x0 = b0 / L00 \
 */ \
   rB00 *= L00; \
   rB01 *= L00; \
   rB02 *= L00; \
   rB03 *= L00; \
   ATL_pfl1W(r_ + ((ldr_)<<2)); \
/* \
 * x1 = (b1 - L10 * x0) / L11 \
 */ \
   rB10 = (rB10 - L10*rB00) * L11;  \
   rB11 = (rB11 - L10*rB01) * L11; \
   rB12 = (rB12 - L10*rB02) * L11; \
   rB13 = (rB13 - L10*rB03) * L11; \
   ATL_pfl1W(r_ + ldr_ +((ldr_)<<2)); \
/* \
 * x2 = (b2 - L20*x0 - L21*x1) / L22 \
 */ \
   rB20 = (rB20 - L20*rB00 - L21*rB10) * L22; \
   rB21 = (rB21 - L20*rB01 - L21*rB11) * L22; \
   rB22 = (rB22 - L20*rB02 - L21*rB12) * L22; \
   rB23 = (rB23 - L20*rB03 - L21*rB13) * L22; \
   ATL_pfl1W(r_ + ((ldr_)<<1)+((ldr_)<<2)); \
/* \
 * x3 = (b3 - L30*x0 - L31*x1 - L32*x2) / L33 \
 */ \
   rB30 = (rB30 - L30*rB00 - L31*rB10 - L32*rB20) * L33; \
   rB31 = (rB31 - L30*rB01 - L31*rB11 - L32*rB21) * L33; \
   ATL_pfl1W(r_ + ldr_+((ldr_)<<1)+((ldr_)<<2)); \
   rB32 = (rB32 - L30*rB02 - L31*rB12 - L32*rB22) * L33; \
   rB33 = (rB33 - L30*rB03 - L31*rB13 - L32*rB23) * L33; \
}  /* complete 4x4 solve block */

#define  ATL_trsmU4(U_, ldu_, r_, ldr_) \
{ \
   const RTYPE U00=(*(U_)); \
   const RTYPE U01=(U_)[ldu_], U11=(U_)[ldu_+1]; \
   const RTYPE U02=(U_)[2*(ldu_)], U12= *(U_+2*(ldu_)+1),  \
               U22 = *(U_+2*(ldu_)+2); \
   const RTYPE U03 = *(U_+3*(ldu_)), U13 = *(U_+3*(ldu_)+1), \
               U23 = *(U_+3*(ldu_)+2), U33 = *(U_+3*(ldu_)+3); \
\
/* \
 * x3 = b3 / U33 \
 */ \
   rB30 *= U33; \
   rB31 *= U33; \
   rB32 *= U33; \
   rB33 *= U33; \
   ATL_pfl1W(r_ + ((ldr_)<<2)); \
/* \
 * x2 = (b2 - U23 * x3) / U22 \
 */ \
   rB20 = (rB20 - U23*rB30) * U22;  \
   rB21 = (rB21 - U23*rB31) * U22; \
   rB22 = (rB22 - U23*rB32) * U22; \
   rB23 = (rB23 - U23*rB33) * U22; \
   ATL_pfl1W(r_ + ldr_+((ldr_)<<2)); \
/* \
 * x1 = (b1 - U12*x2 - U13*x3) / U11 \
 */ \
   rB10 = (rB10 - U12*rB20 - U13*rB30) * U11; \
   rB11 = (rB11 - U12*rB21 - U13*rB31) * U11; \
   rB12 = (rB12 - U12*rB22 - U13*rB32) * U11; \
   rB13 = (rB13 - U12*rB23 - U13*rB33) * U11; \
   ATL_pfl1W(r_ + ((ldr_)<<1)+((ldr_)<<2)); \
/* \
 * x0 = (b0 - U01*x1 - U02*x2 - U03*x3) / U00 \
 */ \
   rB00 = (rB00 - U01*rB10 - U02*rB20 - U03*rB30) * U00; \
   rB01 = (rB01 - U01*rB11 - U02*rB21 - U03*rB31) * U00; \
   ATL_pfl1W(r_ + ldr_+((ldr_)<<1)+((ldr_)<<2)); \
   rB02 = (rB02 - U01*rB12 - U02*rB22 - U03*rB32) * U00; \
   rB03 = (rB03 - U01*rB13 - U02*rB23 - U03*rB33) * U00; \
}  /* complete 4x4 solve block */
#endif

@ROUT ATL_ctrsmKL_rk2
static void ATL_trsmLUN
(
   ATL_CINT  M,   /* size of orig triangular matrix A */
   ATL_CINT  N,   /* number of RHS in B */
   const SCALAR alpha,  /* scale factor for B */
   const TYPE *U, /* McxMc lower matrix A, diag has inverse of original diag */
   TYPE *B,       /* on input, B, on output X, of A*x = b */
   ATL_CINT  ldb, /* leading dim of B */
   TYPE *W        /* McxNRHS workspace with good alignment */
)
{
   const TYPE ral=(*alpha), ial=alpha[1];
   const int ALPHAISREAL = (ial == ATL_rzero);
   ATL_CINT M2 = M+M, ldb2=ldb+ldb;
   #ifdef SCPLX  /* must by aligned for 8 floats */
      ATL_CINT Mc = ((M+7)>>3)<<3;   /* Mceiling = CEIL(m/8)*8 */
   #else         /* must be aligned for 4 doubles */
      ATL_CINT Mc = ((M+3)>>2)<<2;   /* Mceiling = CEIL(m/4)*4 */
   #endif
   ATL_CINT Mc2=Mc+Mc, gap=Mc2-M2;
   ATL_INT i, j, k;

/*
 * Loop over RHS, NRHS RHS at a time
 */
   for (j=0; j < N; j += NRHS, B += NRHS*ldb2)
   {
      const int nb = Mmin(NRHS, N-j);
      TYPE *w = W, *b = B;
      const TYPE *Ac = U + (Mc-2)*Mc2, *a = Ac + Mc2-4;
/*
 *    Copy NRHS RHS into aligned workspace, scaling by alpha as required
 *    Note, will have O(M) reuse
 *    For U, RHS are padded with zero at the top (U is padded with I in 
 *    same region).
 */
      for (k=0; k < nb; k++, w += Mc2-gap, b += ldb2)
      {
         for (i=0; i < gap; i++)
            w[i] = ATL_rzero;
         w += gap;
         if (ALPHAISREAL)
         {
            if (ral  != ATL_rone)
            {
               for (i=0; i < M2; i++)
                  w[i] = ral * b[i];
            }
            else
            {
               for (i=0; i < M2; i++)
                  w[i] = b[i];
            }
         }
         else /* must apply a complex alpha */
         {
            for (i=0; i < M2; i += 2)
            {
               register TYPE rb, ib;

               rb = b[i];
               ib = b[i+1];
               w[i] = rb*ral - ib*ial;
               w[i+1] = rb*ial + ib*ral;
            }
         }
      }
      for (; k < NRHS; k++, w += Mc2)
         for (i=0; i < Mc2; i++)
             w[i] = ATL_rzero;
/*
 *    Completely solve these copied RHSs by looping over entire triangular mat
 */
      b = B + M2;
      w = W + Mc-4;
      for (k=0; k < M; k += 2, b -= 4, w -= 4, a -= (Mc+1)<<2, Ac -= (Mc2<<1))
      {
         ATL_CINT mr = Mmin(2,M-k), mr2=mr+mr;
/*
 *       Solve M=2 NRHS=NRHS TRSM symbolicly
 */
         ATL_trsmU2(a, Mc, w, Mc);
/*
 *       Write solved 2xNRHS block out to original workspace (final answer)
 */
         {
            ATL_INT ii, jj;
            TYPE *bb = b;
            const TYPE *ww = (const TYPE *) w;
            for (jj=0; jj < nb; jj++, ww += Mc2, bb += ldb2)
               for (ii=0; ii < mr2; ii++)
                  bb[ii] = ww[ii];
         }
/*
 *       Subtract off x0 and x1 contribution from rest of B using rank-2 update
 */
         if (M-k-2 > 0)
            ATL_rk2(Mc-k-2, Ac, Mc, W, Mc, w, Mc);
      }         /* end k-loop over U */
   }            /* end loop over RHS */
}  /* end of static func ATL_trsmLUN */

static void ATL_trsmLLN
(
   ATL_CINT  M,   /* size of orig triangular matrix A */
   ATL_CINT  N,   /* number of RHS in B */
   const SCALAR alpha,  /* scale factor for B */
   const TYPE *L, /* McxMc lower matrix A, diag has inverse of original diag */
   TYPE *B,       /* on input, B, on output X, of A x = b */
   ATL_CINT  ldb, /* leading dim of B */
   TYPE *W        /* MxNRHS workspace with good alignment */
)
{
   const TYPE ral=(*alpha), ial=alpha[1];
   const int ALPHAISREAL = (ial == ATL_rzero);
   ATL_CINT M2 = M+M, ldb2=ldb+ldb;
   #ifdef SCPLX  /* must by aligned for 8 floats */
      ATL_CINT Mc = ((M+7)>>3)<<3;   /* Mceiling = CEIL(m/8)*8 */
   #else         /* must be aligned for 4 doubles */
      ATL_CINT Mc = ((M+3)>>2)<<2;   /* Mceiling = CEIL(m/4)*4 */
   #endif
   ATL_CINT Mc2=Mc+Mc;
   ATL_INT i, j, k;

/*
 * Loop over RHS, NRHS RHS at a time
 */
   for (j=0; j < N; j += NRHS, B += NRHS*ldb2)
   {
      const int nb = Mmin(NRHS, N-j);
      TYPE *w = W, *b = B;
      const TYPE *a;
/*
 *    Copy NRHS RHS into aligned workspace, scaling by alpha as required
 *    Note, will have O(M) reuse
 */
      for (k=0; k < nb; k++, w += Mc2, b += ldb2)
      {
         if (ALPHAISREAL)
         {
            if (ral  != ATL_rone)
            {
               for (i=0; i < M2; i++)
                  w[i] = ral * b[i];
            }
            else
            {
               for (i=0; i < M2; i++)
                  w[i] = b[i];
            }
         }
         else /* must apply a complex alpha */
         {
            for (i=0; i < M2; i += 2)
            {
               register TYPE rb, ib;

               rb = b[i];
               ib = b[i+1];
               w[i] = rb*ral - ib*ial;
               w[i+1] = rb*ial + ib*ral;
            }
         }
         for (; i < Mc2; i++)
             w[i] = ATL_rzero;
      }
      for (; k < NRHS; k++, w += Mc2)
         for (i=0; i < Mc2; i++)
             w[i] = ATL_rzero;
/*
 *    Completely solve these copied RHSs by looping over entire triangular mat
 */
      b = B;
      w = W;
      a = L;
      for (k=0; k < M; k += 2, b += 4, w += 4, a += (Mc+1)<<2)
      {
         ATL_CINT mr = Mmin(2,M-k), mr2=mr+mr;
/*
 *       Solve M=2 NRHS=NRHS TRSM symbolically
 */
         ATL_trsmL2(a, Mc, w, Mc);
/*
 *       Write solved 2xNRHS block out to original workspace (final answer)
 */
         {
            ATL_INT ii, jj;
            TYPE *bb = b;
            const TYPE *ww = (const TYPE *) w;
            for (jj=0; jj < nb; jj++, ww += Mc2, bb += ldb2)
               for (ii=0; ii < mr2; ii++)
                  bb[ii] = ww[ii];
         }
/*
 *       Subtract off x0 and x1 contribution from rest of B using rank-2 update
 */
         if (M-k-2 > 0)
            ATL_rk2(Mc-k-2, a+4, Mc, w, Mc, w+4, Mc);
      }         /* end k-loop over L */
   }            /* end loop over RHS */
}  /* end of static func ATL_trsmLLN */
@ROUT ATL_trsmKL_rk4
static void ATL_trsmLLN
(
   ATL_CINT  M,   /* size of orig triangular matrix A */
   ATL_CINT  N,   /* number of RHS in B */
   const SCALAR alpha,  /* scale factor for B */
   const TYPE *A, /* MxM lower matrix A, diag has inverse of original diag */
   TYPE *B,       /* on input, B, on output X, of A x = b */
   ATL_CINT  ldb, /* leading dim of B */
   TYPE *W        /* Mx4 workspace with good alignment */
)
{
   int j;
   ATL_CINT M4 = ((M+3)>>2)<<2;

   #define lda M4
/*
 * Loop over RHS, NRHS RHS at a time
 */
   for (j=0; j < N; j += NRHS, B += NRHS*ldb)
   {
      const int nb = Mmin(NRHS, N-j);
      int k, i;
      TYPE *w = W, *b = B;
      const TYPE *a;
/*
 *    Copy NRHS RHS to aligned workspace and scale if necessary, alpha cannot be
 *    zero, because this is handled as a special case at top
 */
      for (k=0; k < nb; k++, w += M4, b += ldb)
      {
         if (alpha != 1.0)
         {
            for (i=0; i < M; i++)
               w[i] = alpha * b[i];
         }
         else
         {
            for (i=0; i < M; i++)
               w[i] = b[i];
         }
         for (; i < M4; i++)
             w[i] = ATL_rzero;
      }
      for (; k < NRHS; k++, w += M4)
         for (i=0; i < M4; i++)
             w[i] = ATL_rzero;
/*
 *    Completely solve these RHSs by looping over entire triangular matrix
 */
      b = B;
      w = W;
      a = A;
      for (k=0; k < M; k += 4, b += 4, w += 4, a += (lda+1)<<2)
      {
         ATL_CINT mr = Mmin(4,M-k);
         RTYPE rB00 = *w, rB10=w[1], rB20=w[2], rB30=w[3];
         RTYPE rB01=w[M4], rB11=w[M4+1], rB21=w[M4+2], rB31=w[M4+3];
         #if NRHS > 2
            RTYPE rB02=w[2*M4],rB12=w[2*M4+1],rB22=w[2*M4+2],rB32=w[2*M4+3];
         #endif
         #if NRHS > 3
         RTYPE rB03=w[3*M4],rB13=w[3*M4+1],rB23=w[3*M4+2],rB33=w[3*M4+3];
         #endif
/*
 *       Solve M=4 NRHS=4 TRSM symbolically
 */
         ATL_trsmL4(a, lda, b, ldb);
/*
 *       Write solved 4x4 block out to original workspace (final answer)
 *       Handle most common case with only one if
 */
         if (mr == 4 && nb == NRHS)
         {
            *b = rB00;
            b[1] = rB10;
            b[2] = rB20;
            b[3] = rB30;
            b[ldb] = rB01;
            b[ldb+1] = rB11;
            b[ldb+2] = rB21;
            b[ldb+3] = rB31;
            #if NRHS > 2
               b[ldb+ldb] = rB02;
               b[ldb+ldb+1] = rB12;
               b[ldb+ldb+2] = rB22;
               b[ldb+ldb+3] = rB32;
            #endif
            #if NRHS > 3
               b[(ldb<<1)+ldb] = rB03;
               b[(ldb<<1)+ldb+1] = rB13;
               b[(ldb<<1)+ldb+2] = rB23;
               b[(ldb<<1)+ldb+3] = rB33;
            #endif
         }
         else
         {
            switch(mr)
            {
            case 4:
               b[3] = rB30;
            case 3:
               b[2] = rB20;
            case 2:
               b[1] = rB10;
            case 1:
               *b = rB00;
            }
            if (nb > 1)
            {
               switch(mr)
               {
               case 4:
                  b[ldb+3] = rB31;
               case 3:
                  b[ldb+2] = rB21;
               case 2:
                  b[ldb+1] = rB11;
               case 1:
                  b[ldb] = rB01;
               }
               #if NRHS > 2
               if (nb > 2)
               {
                  switch(mr)
                  {
                  case 4:
                     b[ldb+ldb+3] = rB32;
                  case 3:
                     b[ldb+ldb+2] = rB22;
                  case 2:
                     b[ldb+ldb+1] = rB12;
                  case 1:
                     b[ldb+ldb] = rB02;
                  }
                  #if NRHS > 3
                  if (nb > 3)
                  {
                     switch(mr)
                     {
                     case 4:
                        b[(ldb<<1)+ldb+3] = rB33;
                     case 3:
                        b[(ldb<<1)+ldb+2] = rB23;
                     case 2:
                        b[(ldb<<1)+ldb+1] = rB13;
                     case 1:
                        b[(ldb<<1)+ldb] = rB03;
                     }
                  }
                  #endif
               }
               #endif
            }
         }
/*
 *       Subtract off x0-x4 contribution from rest of B using rank-4 update
 */
         #if ATL_BINWRK
            if (M-k-4 > 0)
            {
               *w = rB00;
               w[1] = rB10;
               w[2] = rB20;
               w[3] = rB30;
               w[M4] = rB01;
               w[M4+1] = rB11;
               w[M4+2] = rB21;
               w[M4+3] = rB31;
               #if NRHS > 2
                  w[2*M4] = rB02;
                  w[2*M4+1] = rB12;
                  w[2*M4+2] = rB22;
                  w[2*M4+3] = rB32;
               #endif
               #if NRHS > 3
                  w[3*M4] = rB03;
                  w[3*M4+1] = rB13;
                  w[3*M4+2] = rB23;
                  w[3*M4+3] = rB33;
               #endif
               ATL_rk4(M4-k-4, a+4, lda, w, M4, w+4, M4);
            }
         #else
            ATL_rk4(M4-k-4, a+4, lda, w+4, M4);
         #endif
      }     /* end k-loop that loops through L */
   }        /* end j-loop over RHS */
   #undef lda
}           /* end routine */

static void ATL_trsmLUN
(
   ATL_CINT  M,         /* size of orig triangular matrix A */
   ATL_CINT  N,         /* number of RHS in B */
   const SCALAR alpha,  /* scale factor for B */
   const TYPE *A, /* M4xM4 Upper matrix A, diag has inverse of original diag */
   TYPE *B,             /* on input, B, on output X, of A x = b */
   ATL_CINT  ldb,       /* leading dim of B */
   TYPE *W              /* M4x4 workspace with good alignment */
)
{
   int j;
   ATL_CINT M4 = ((M+3)>>2)<<2, mr = M4-M;
/*
 * Loop over RHS, NRHS RHS at a time
 */
   for (j=0; j < N; j += NRHS, B += NRHS*ldb)
   {
      const int nb = Mmin(NRHS, N-j);
      int k, i;
      TYPE *w = W, *b = B;
      const TYPE *Ac = A + (M4-4)*M4, *a = Ac + M4-4;
/*
 *    Copy NRHS RHS to aligned workspace and scale if necessary, alpha cannot be
 *    zero, because this is handled as a special case at top
 */
      for (k=0; k < nb; k++, w += M4, b += ldb)
      {
         for (i=0; i < mr; i++)
             w[i] = ATL_rzero;
         if (alpha != 1.0)
         {
            for (; i < M4; i++)
               w[i] = alpha * b[i-mr];
         }
         else
         {
            for (; i < M4; i++)
               w[i] = b[i-mr];
         }
      }
      for (; k < NRHS; k++, w += M4)
         for (i=0; i < M4; i++)
             w[i] = ATL_rzero;
/*
 *    Completely solve these RHSs by looping over entire triangular matrix
 */
      b = B + M;
      w = W + M4-4;
      for (k=0; k < M; k += 4, w -= 4, a -= (M4+1)<<2, Ac -= M4<<2)
      {
         ATL_CINT mr = Mmin(4,M-k);
         RTYPE rB00 = *w, rB10=w[1], rB20=w[2], rB30=w[3];
         RTYPE rB01=w[M4], rB11=w[M4+1], rB21=w[M4+2], rB31=w[M4+3];
         #if NRHS > 2
            RTYPE rB02=w[2*M4],rB12=w[2*M4+1],rB22=w[2*M4+2],rB32=w[2*M4+3];
         #endif
         #if NRHS > 3
            RTYPE rB03=w[3*M4],rB13=w[3*M4+1],rB23=w[3*M4+2],rB33=w[3*M4+3];
         #endif
/*
 *       Solve M=4 NRHS=4 TRSM symbolically
 */
         b -= mr;
         ATL_trsmU4(a, M4, b, ldb);
/*
 *       Write solved 4x4 block out to original workspace (final answer)
 *       Handle most common case with only one if
 */
         if (mr == 4 && nb == NRHS)
         {
            *b = rB00;
            b[1] = rB10;
            b[2] = rB20;
            b[3] = rB30;
            b[ldb] = rB01;
            b[ldb+1] = rB11;
            b[ldb+2] = rB21;
            b[ldb+3] = rB31;
            #if NRHS > 2
               b[ldb+ldb] = rB02;
               b[ldb+ldb+1] = rB12;
               b[ldb+ldb+2] = rB22;
               b[ldb+ldb+3] = rB32;
            #endif
            #if NRHS > 3
               b[(ldb<<1)+ldb] = rB03;
               b[(ldb<<1)+ldb+1] = rB13;
               b[(ldb<<1)+ldb+2] = rB23;
               b[(ldb<<1)+ldb+3] = rB33;
            #endif
         }
         else
         {
            switch(mr)
            {
            case 1:
               *b = rB30;
               break;
            case 2:
               *b = rB20;
               b[1] = rB30;
               break;
            case 3:
               *b = rB10;
               b[1] = rB20;
               b[2] = rB30;
               break;
            case 4:
               *b = rB00;
               b[1] = rB10;
               b[2] = rB20;
               b[3] = rB30;
               break;
            }
            if (nb > 1)
            {
               switch(mr)
               {
               case 1:
                  b[ldb] = rB31;
                  break;
               case 2:
                  b[ldb] = rB21;
                  b[ldb+1] = rB31;
                  break;
               case 3:
                  b[ldb] = rB11;
                  b[ldb+1] = rB21;
                  b[ldb+2] = rB31;
                  break;
               case 4:
                  b[ldb] = rB01;
                  b[ldb+1] = rB11;
                  b[ldb+2] = rB21;
                  b[ldb+3] = rB31;
                  break;
               }
               #if NRHS > 2
               if (nb > 2)
               {
                  switch(mr)
                  {
                  case 1:
                     b[ldb+ldb] = rB32;
                     break;
                  case 2:
                     b[ldb+ldb] = rB22;
                     b[ldb+ldb+1] = rB32;
                     break;
                  case 3:
                     b[ldb+ldb] = rB12;
                     b[ldb+ldb+1] = rB22;
                     b[ldb+ldb+2] = rB32;
                     break;
                  case 4:
                     b[ldb+ldb] = rB02;
                     b[ldb+ldb+1] = rB12;
                     b[ldb+ldb+2] = rB22;
                     b[ldb+ldb+3] = rB32;
                     break;
                  }
                  #if NRHS > 3
                  if (nb > 3)
                  {
                     ATL_CINT ldb3 = ldb+(ldb<<1);
                     switch(mr)
                     {
                     case 1:
                        b[ldb3] = rB33;
                        break;
                     case 2:
                        b[ldb3] = rB23;
                        b[ldb3+1] = rB33;
                        break;
                     case 3:
                        b[ldb3] = rB13;
                        b[ldb3+1] = rB23;
                        b[ldb3+2] = rB33;
                        break;
                     case 4:
                        b[ldb3] = rB03;
                        b[ldb3+1] = rB13;
                        b[ldb3+2] = rB23;
                        b[ldb3+3] = rB33;
                        break;
                     }
                  }
                  #endif
               }
               #endif
            }
         }
/*
 *       Subtract off x0-x4 contribution from rest of B using rank-4 update
 */
         #if ATL_BINWRK
            if (M-k-4 > 0)
            {
               *w = rB00;
               w[1] = rB10;
               w[2] = rB20;
               w[3] = rB30;
               w[M4] = rB01;
               w[M4+1] = rB11;
               w[M4+2] = rB21;
               w[M4+3] = rB31;
               #if NRHS > 2
                  w[2*M4] = rB02;
                  w[2*M4+1] = rB12;
                  w[2*M4+2] = rB22;
                  w[2*M4+3] = rB32;
               #endif
               #if NRHS > 3
                  w[3*M4] = rB03;
                  w[3*M4+1] = rB13;
                  w[3*M4+2] = rB23;
                  w[3*M4+3] = rB33;
               #endif
               ATL_rk4(M4-k-4, Ac, M4, w, M4, W, M4);
            }
         #else
            ATL_rk4(M4-k-4, Ac, M4, W, M4);
         #endif
      }     /* end k-loop that loops through L */
   }        /* end j-loop over RHS */
}           /* end routine */
@ROUT ATL_trsmKR_rk4
static void ATL_trsmRLN
(
   ATL_CINT  M,   /* size of orig triangular matrix A */
   ATL_CINT  N,   /* number of RHS in B */
   const TYPE *A, /* M4xM4 lower matrix A, diag has inverse of original diag */
   TYPE *W        /* M4xN, on input padded B, on output, padded X */
)
{
   int j;
   ATL_CINT M4 = ((M+3)>>2)<<2;

   #define lda M4
/*
 * Loop over RHS, NRHS RHS at a time
 */
   for (j=0; j < N; j += NRHS, W += NRHS*M4)
   {
      const int nb = Mmin(NRHS, N-j);
      int k, i;
      TYPE *w = W;
      const TYPE *a;
/*
 *    Completely solve these RHSs by looping over entire triangular matrix
 */
      w = W;
      a = A;
      for (k=0; k < M; k += 4, w += 4, a += (lda+1)<<2)
      {
         ATL_CINT mr = Mmin(4,M-k);
         RTYPE rB00 = *w, rB10=w[1], rB20=w[2], rB30=w[3];
         RTYPE rB01=w[M4], rB11=w[M4+1], rB21=w[M4+2], rB31=w[M4+3];
         #if NRHS > 2
            RTYPE rB02=w[2*M4],rB12=w[2*M4+1],rB22=w[2*M4+2],rB32=w[2*M4+3];
         #endif
         #if NRHS > 3
            RTYPE rB03=w[3*M4],rB13=w[3*M4+1],rB23=w[3*M4+2],rB33=w[3*M4+3];
         #endif
/*
 *       Solve M=4 NRHS=4 TRSM symbolically
 */
         ATL_trsmL4(a, lda, w, M4);
/*
 *       Write solved 4xNRHS block out to workspace (final answer)
 */
         *w = rB00;
         w[1] = rB10;
         w[2] = rB20;
         w[3] = rB30;
         w[M4] = rB01;
         w[M4+1] = rB11;
         w[M4+2] = rB21;
         w[M4+3] = rB31;
         #if NRHS > 2
            w[M4+M4] = rB02;
            w[M4+M4+1] = rB12;
            w[M4+M4+2] = rB22;
            w[M4+M4+3] = rB32;
         #endif
         #if NRHS > 3
            w[3*M4] = rB03;
            w[3*M4+1] = rB13;
            w[3*M4+2] = rB23;
            w[3*M4+3] = rB33;
         #endif
/*
 *       Subtract off x0-x4 contribution from rest of B using rank-4 update
 */
         #if ATL_BINWRK
            ATL_rk4(M4-k-4, a+4, lda, w, M4, w+4, M4);
         #else
            ATL_rk4(M4-k-4, a+4, lda, w+4, M4);
         #endif
      }     /* end k-loop that loops through L */
   }        /* end j-loop over RHS */
   #undef lda
}           /* end routine */

static void ATL_trsmRUN
(
   ATL_CINT  M,         /* size of orig triangular matrix A */
   ATL_CINT  N,         /* number of RHS in W */
   const TYPE *A,       /* M4xM4 Upper matrix A, diag is inverted */
   TYPE *W              /* M4xN workspace with good alignment */
)
{
   int j;
   ATL_CINT M4 = ((M+3)>>2)<<2, mr = M4-M;
/*
 * Loop over RHS, NRHS RHS at a time
 */
   for (j=0; j < N; j += NRHS, W += NRHS*M4)
   {
      const int nb = Mmin(NRHS, N-j);
      int k, i;
      TYPE *w = W;
      const TYPE *Ac = A + (M4-4)*M4, *a = Ac + M4-4;
/*
 *    Completely solve these RHSs by looping over entire triangular matrix
 */
      w = W + M4-4;
      for (k=0; k < M; k += 4, w -= 4, a -= (M4+1)<<2, Ac -= M4<<2)
      {
         ATL_CINT mr = Mmin(4,M-k);
         RTYPE rB00 = *w, rB10=w[1], rB20=w[2], rB30=w[3];
         RTYPE rB01=w[M4], rB11=w[M4+1], rB21=w[M4+2], rB31=w[M4+3];
         #if NRHS > 2
            RTYPE rB02=w[2*M4],rB12=w[2*M4+1],rB22=w[2*M4+2],rB32=w[2*M4+3];
         #endif
         #if NRHS > 3
            RTYPE rB03=w[3*M4],rB13=w[3*M4+1],rB23=w[3*M4+2],rB33=w[3*M4+3];
         #endif
/*
 *       Solve M=4 NRHS=4 TRSM symbolically
 */
         ATL_trsmU4(a, M4, w, M4);
/*
 *       Store solved 4xNRHS elts of X to workspace
 */
         *w = rB00;
         w[1] = rB10;
         w[2] = rB20;
         w[3] = rB30;
         w[M4] = rB01;
         w[M4+1] = rB11;
         w[M4+2] = rB21;
         w[M4+3] = rB31;
         #if NRHS > 2
            w[2*M4] = rB02;
            w[2*M4+1] = rB12;
            w[2*M4+2] = rB22;
            w[2*M4+3] = rB32;
         #endif
         #if NRHS > 3
            w[3*M4] = rB03;
            w[3*M4+1] = rB13;
            w[3*M4+2] = rB23;
            w[3*M4+3] = rB33;
         #endif
/*
 *       Subtract off x0-x4 contribution from rest of B using rank-4 update
 */
         #if ATL_BINWRK
            ATL_rk4(M4-k-4, Ac, M4, w, M4, W, M4);
         #else
            ATL_rk4(M4-k-4, Ac, M4, W, M4);
         #endif
      }     /* end k-loop that loops through L */
   }        /* end j-loop over RHS */
}           /* end routine */
@ROUT ATL_ctrsmKL_rk2
ATL_SINLINE void trU2Lc
   (enum ATLAS_DIAG Diag, ATL_CINT N, const TYPE *U, ATL_CINT ldu, 
    TYPE *L, ATL_CINT ldl)
/*
 * reflects upper part of U into lower part of L with conjugation,
 * Lower, right part of L is padded to ldl with I to reach ldl size
 */
{
   ATL_CINT N2=N+N, ldU=ldu+ldu, ldL=ldl+ldl;
   ATL_INT i, j;
   const TYPE *Uc = U;

   for (j=0; j < N2; j += 2, Uc += ldU)
   {
      TYPE *Lr = L + j;
      for (i=0; i < j; i += 2, Lr += ldL)
      {
         *Lr = Uc[i];
         Lr[1] = -Uc[i+1];
      }
      if (Diag == AtlasUnit)
      {
         *Lr = ATL_rone;
         Lr[1] = ATL_rzero;
      }
      else
      {
         *Lr = Uc[j];
         Lr[1] = -Uc[j+1];
         Mjoin(PATL,cplxinvert)(1, Lr, 1, Lr, 1);
      }
   }
/*
 * Pad left and lower portion of L if ldl > N
 */
  if (ldl > N)
  {
/* 
 *    Pad the last ldl-N rows of L with zeros in the first N columns of L
 */
      for (j=0; j < N2; j += 2, L += ldL)
      {
         for (i=N2; i < ldL; i++)
            L[i] = ATL_rzero;
      }
  
/* 
 *    Pad the last ldl-N columns of L with the identity matrix
 */
      for (; j < ldL; j += 2, L += ldL)
      {
         L[j] = ATL_rone;
         L[j+1] = ATL_rzero;
         for (i=j+2; i < ldL; i++)
            L[i] = ATL_rzero;
      }
   }
}
ATL_SINLINE void trU2L
   (enum ATLAS_DIAG Diag, ATL_CINT N, const TYPE *U, ATL_CINT ldu, 
    TYPE *L, ATL_CINT ldl)
/*
 * reflects upper part of U into lower part of L, 
 * Lower, right part of L is padded to ldl with I to reach ldl size
 */
{
   ATL_CINT N2=N+N, ldU=ldu+ldu, ldL=ldl+ldl;
   ATL_INT i, j;
   const TYPE *Uc = U;

   for (j=0; j < N2; j += 2, Uc += ldU)
   {
      TYPE *Lr = L + j;
      for (i=0; i < j; i += 2, Lr += ldL)
      {
         *Lr = Uc[i];
         Lr[1] = Uc[i+1];
      }
      if (Diag == AtlasUnit)
      {
         *Lr = ATL_rone;
         Lr[1] = ATL_rzero;
      }
      else
         Mjoin(PATL,cplxinvert)(1, (TYPE*)(Uc+j), 1, Lr, 1);
   }
/*
 * Pad left and lower portion of L if ldl > N
 */
  if (ldl > N)
  {
/* 
 *    Pad the last ldl-N rows of L with zeros in the first N columns of L
 */
      for (j=0; j < N2; j += 2, L += ldL)
      {
         for (i=N2; i < ldL; i++)
            L[i] = ATL_rzero;
      }
  
/* 
 *    Pad the last ldl-N columns of L with the identity matrix
 */
      for (; j < ldL; j += 2, L += ldL)
      {
         L[j] = ATL_rone;
         L[j+1] = ATL_rzero;
         for (i=j+2; i < ldL; i++)
            L[i] = ATL_rzero;
      }
   }
}

@ROUT ATL_trsmKL_rk4 ATL_trsmKR_rk4

ATL_SINLINE void trL2U
   (ATL_CINT N, const TYPE *L, ATL_CINT ldl, TYPE *U, ATL_CINT ldu)
/*
 * reflects lower part of L into upper part of U
 */
{
   const TYPE *Lc=L;
   ATL_INT i, j;

   for (j=0; j < N; j++, Lc += ldl)
   {
      TYPE *Ur = U + j;
      for (i=j; i < N; i++)
         U[j+i*ldu] = Lc[i];
   }
}

ATL_SINLINE void trU2L
   (ATL_CINT N, const TYPE *U, ATL_CINT ldu, TYPE *L, ATL_CINT ldl)
/*
 * reflects upper part of U into lower part of L
 */
{
   ATL_INT i, j;
   const TYPE *Uc = U;

   for (j=0; j < N; j++, Uc += ldu)
   {
      TYPE *Lr = L + j;
      for (i=0; i <= j; i++, Lr += ldl)
         *Lr = Uc[i];
   }
}

/*
 * Copy original U to aligned workspace, invert diagonal elts, pad wt I
 * Padding is at top of upper triangular matrix
 */
static void cpypadU
(
   enum ATLAS_DIAG Diag, 
   ATL_CINT N,                  /* size of triangular matrix A */
   const TYPE *A,               /* lower triangular matrix */
   ATL_CINT lda,                /* leading dim of A */
   TYPE *a,                     /* cpy of A, padded to N4 with I */
   ATL_CINT N4                  /* leading dim of A */
)
{
   int i, j;
   const int mr = N4-N;

   for (j=0; j < mr; j++, a += N4)
   {
      for (i=0; i < N4; i++)
         a[i] = ATL_rzero;
      a[j] = ATL_rone;
   }
   for (; j < N4; j++, a += N4, A += lda)
   {
      for (i=0; i < mr; i++)
         a[i] = ATL_rzero;
      for (; i < j; i++)
         a[i] = A[i-mr];
      a[j] = (Diag == AtlasNonUnit) ? 1.0 / A[j-mr] : ATL_rone;
   }
}

/*
 * Copy original L to aligned workspace, invert diagonal elts, pad wt I
 */
static void cpypadL
(
   enum ATLAS_DIAG Diag, 
   ATL_CINT N,                  /* size of triangular matrix A */
   const TYPE *A,               /* lower triangular matrix */
   ATL_CINT lda,                /* leading dim of A */
   TYPE *a,                     /* cpy of A, padded to N4 with I */
   ATL_CINT N4                  /* leading dim of A */

)
{
   int i, j;

   for (j=0; j < N; j++, a += N4, A += lda)
   {
      a[j] = (Diag == AtlasNonUnit) ? 1.0 / A[j] : ATL_rone;
      for (i=j+1; i < N; i++)
         a[i] = A[i];
      for (; i < N4; i++)
         a[i] = ATL_rzero;
   }
   for (; j < N4; j++, a += N4)
   {
      for (i=0; i < N4; i++)
         a[i] = ATL_rzero;
      a[j] = ATL_rone;
   }
}
@ROUT ATL_ctrsmKL_rk2
/*
 * Copy original U to aligned workspace, invert diagonal elts, pad wt I
 * U is padded with I at top left of matrix to fit Nc
 */
static void cpypadU
(
   enum ATLAS_DIAG Diag, 
   ATL_CINT N,                  /* size of triangular matrix A */
   const TYPE *A,               /* lower triangular matrix */
   ATL_CINT lda,                /* leading dim of A */
   TYPE *a,                     /* cpy of A, padded to N4 with I */
   ATL_CINT Nc                  /* leading dim of A, how much to pad */
)
{
   int i, j;
   ATL_CINT lda2 = lda+lda, N2=N+N, Nc2=Nc+Nc;

/* 
 * Pad Nc2-N upper, left of U with I
 */
   if (Nc != N)
   {
      const int nn = Nc2-N2;
      TYPE *aa;
      for (j=0; j < nn; j += 2, a += Nc2)
      {
         for (i=0; i < Nc2; i++)
            a[i] = ATL_rzero;
         a[j] = ATL_rone;
         a[j+1] = ATL_rzero;
      }
/*
 *    Now zero first few rows of each column above that actual U
 */
      aa = a;
      for (j=0; j < N; j++, aa += Nc2)
      {
         for (i=0; i < nn; i++)
            aa[i] = ATL_rzero;
      }
      a += nn;  /* a now pts to place to copy actual U */
   }
/*
 * Copy unpadded portion to U
 */
   for (j=0; j < N2; j += 2, a += Nc2, A += lda2)
   {
      for (i=0; i < j; i++)
         a[i] = A[i];
      if (Diag == AtlasNonUnit)
         Mjoin(PATL,cplxinvert)(1, (TYPE*)(A+j), 1, a+j, 1);
      else
      {
         a[j] = ATL_rone;
         a[j+1] = ATL_rzero;
      }
   }
}
/*
 * Copy original L to aligned workspace, invert diagonal elts, pad wt I
 */
static void cpypadL
(
   enum ATLAS_DIAG Diag, 
   ATL_CINT N,                  /* size of triangular matrix A */
   const TYPE *A,               /* lower triangular matrix */
   ATL_CINT lda,                /* leading dim of A */
   TYPE *a,                     /* cpy of A, padded to N4 with I */
   ATL_CINT Nc                  /* leading dim of A, how much to pad */
)
{
   int i, j;
   ATL_CINT lda2 = lda+lda, N2=N+N, Nc2=Nc+Nc;

   for (j=0; j < N; j++, a += Nc2, A += lda2)
   {
      ATL_CINT j2 = j+j;

      if (Diag == AtlasNonUnit)
         Mjoin(PATL,cplxinvert)(1, (TYPE*)(A+j2), 1, a+j2, 1);
      else
      {
         a[j2] = ATL_rone;
         a[j2+1] = ATL_rzero;
      }
      for (i=j2+2; i < N2; i++)
         a[i] = A[i];
      for (; i < Nc2; i++)
         a[i] = ATL_rzero;
   }
   for (; j < Nc; j++, a += Nc2)
   {
      for (i=0; i < Nc2; i++)
         a[i] = ATL_rzero;
      a[j+j] = ATL_rone;
   }
}
int Mjoin(PATL,trsmKL_rk2)   /* returns 0 on success */
(
   enum ATLAS_SIDE Side,
   enum ATLAS_UPLO Uplo,
   enum ATLAS_TRANS TA,
   enum ATLAS_DIAG Diag,
   ATL_CINT  M,   /* size of triangular matrix A */
   ATL_CINT  N,   /* number of RHS in B */
   const SCALAR alpha,  /* scale factor for B */
   const TYPE *A0, /* MxM lower matrix A, diag has inverse of original diag */
   ATL_CINT  lda0,
   TYPE *B,       /* on input, B, on output X, of A x = b */
   ATL_CINT  ldb  /* leading dim of B */
)
{
   void *vp;
   const TYPE *A = A0;
   TYPE *a, *w, *t=NULL;
   #ifdef SCPLX
      ATL_CINT Mc = ((M+7)>>3)<<3;  /* ceiling */
   #else
      ATL_CINT Mc = ((M+3)>>2)<<2;
   #endif
   ATL_CINT Mc2 = Mc+Mc;
   ATL_INT lda = lda0;
   int UPPER = (Uplo == AtlasUpper);

   ATL_assert(Side == AtlasLeft);
   vp = malloc(ATL_MulBySize(Mc*Mc+Mc*NRHS)+2*ATL_Cachelen);
   if (!vp)
      return(1);

   a = ATL_AlignPtr(vp);
   w = a + Mc*Mc2;
   w = ATL_AlignPtr(w);
   if (UPPER)
   {
      if (TA == AtlasNoTrans)
      {
         cpypadU(Diag, M, A, lda, a, Mc);
         ATL_trsmLUN(M, N, alpha, a, B, ldb, w);
      }
      else if (TA == AtlasTrans)
      {
         trU2L(Diag, M, A0, lda0, a, Mc);
         ATL_trsmLLN(M, N, alpha, a, B, ldb, w);
      }
      else  /* TA == AtlasConjTrans */
      {
         trU2Lc(Diag, M, A0, lda0, a, Mc);
         ATL_trsmLLN(M, N, alpha, a, B, ldb, w);
      }
   }
   else  /* Lower */
   {
      if (TA == AtlasNoTrans)
      {
         cpypadL(Diag, M, A, lda, a, Mc);
         ATL_trsmLLN(M, N, alpha, a, B, ldb, w);
      }
      else if (TA == AtlasTrans)
      {
         ATL_assert(0);
      }
      else  /* TA == AtlasConjTrans */
      {
         ATL_assert(0);
      }
   }
   free(vp);
   return(0);
}
@ROUT ATL_trsmKL_rk4
int Mjoin(PATL,trsmKL_rk4)   /* returns 0 on success */
(
   enum ATLAS_SIDE Side,
   enum ATLAS_UPLO Uplo,
   enum ATLAS_TRANS TA,
   enum ATLAS_DIAG Diag,
   ATL_CINT  M,   /* size of triangular matrix A */
   ATL_CINT  N,   /* number of RHS in B */
   const SCALAR alpha,  /* scale factor for B */
   const TYPE *A0, /* MxM lower matrix A, diag has inverse of original diag */
   ATL_CINT  lda0,
   TYPE *B,       /* on input, B, on output X, of A x = b */
   ATL_CINT  ldb  /* leading dim of B */
)
{
   void *vp;
   const TYPE *A = A0;
   TYPE *a, *w, *t=NULL;
   ATL_CINT M4 = ((M+3)>>2)<<2;
   ATL_INT lda = lda0;
   int UPPER = (Uplo == AtlasUpper);

   if (TA == AtlasTrans)
   {
      t = malloc(M*M*sizeof(TYPE));
      ATL_assert(t);
      if (UPPER)
         trU2L(M, A0, lda0, t, M);
      else
         trL2U(M, A0, lda0, t, M);
      UPPER = !UPPER;
      A = (const TYPE *) t;
      lda = M;
   }
   vp = malloc(sizeof(TYPE)*(M4*M4+M4*NRHS)+2*ATL_Cachelen);
   if (!vp)
      return(1);

   a = ATL_AlignPtr(vp);
   w = a + M4*M4;
   w = ATL_AlignPtr(w);
   if (!UPPER)
   {
      int j, i;
      TYPE *ap=a;
      cpypadL(Diag, M, A, lda, a, M4);
      if (t)
         free(t);
      ATL_trsmLLN(M, N, alpha, a, B, ldb, w);
   }
   else  /* Uplo == AtlasUpper */
   {
      int j, i;
      const int mr = M4-M;
      TYPE *ap=a;
      cpypadU(Diag, M, A, lda, a, M4);
      if (t)
         free(t);
      ATL_trsmLUN(M, N, alpha, a, B, ldb, w);
   }
   free(vp);
   return(0);
}
@ROUT ATL_trsmKR_rk4
/* 
 * This routine computes X * op(A) = B by first computing:
 *    op(A)^T * X^T = B^T
 * and then transposing the answer back out.
 */
int Mjoin(PATL,trsmKR_rk4)
(
   enum ATLAS_SIDE Side,
   enum ATLAS_UPLO Uplo,
   enum ATLAS_TRANS TA,
   enum ATLAS_DIAG Diag,
   ATL_CINT  M,         /* number of RHS in B */
   ATL_CINT  N,         /* size of triangular matrix A */ 
   const SCALAR alpha,  /* scale factor for B */
   const TYPE *A,       /* MxM triangular matrix A */
   ATL_CINT  lda,
   TYPE *B,             /* on input, B, on output X, of A x = b */
   ATL_CINT  ldb        /* leading dim of B */
)
{
   void *vp;
   TYPE *a, *w, *b;
   enum ATLAS_TRANS TRANSA;
   ATL_CINT N4 = ((N+3)>>2)<<2, nr = N4-N;
   ATL_CINT Mc = Mmin(((M+NRHS-1)/NRHS)*NRHS, 36);
   int UPPER = (Uplo == AtlasUpper);
   const int TRANS = (TA == AtlasTrans), UNIT=(Diag == AtlasUnit);
   ATL_INT i, j;

   vp = malloc((Mc*N4+N4*N4)*sizeof(TYPE) + 2*ATL_Cachelen);
   if (!vp) 
      return(1);
   a = ATL_AlignPtr(vp);
   w = a+N4*N4;
   w = ATL_AlignPtr(w);
/*
 * If matrix is already transposed, keep Uplo same and just copy to space
 * and invert the diagonal, and swap the transpose setting
 */
   if (TRANS)
   {
     if (UPPER)
        cpypadU(Diag, N, A, lda, a, N4);
     else
        cpypadL(Diag, N, A, lda, a, N4);
   }
/* 
 * If the matrix is presently in NoTranspose format, transpose it, which
 * will swap Uplo setting
 */
   else
   {
      if (UPPER)
      {
         TYPE *c=a;
         trU2L(N, A, lda, a, N4);  /* transpose A into lower-triangular a */
/*
 *       invert diagonal and pad N4-N gap at bottom of Lower matrix
 */
         for (j=0; j < N; j++, c += N4)
         {
            c[j] = (UNIT) ? ATL_rone : ATL_rone / c[j];
            for (i=N; i < N4; i++)
               c[i] = ATL_rzero;
         }
/*
 *       Pad last N4-N columns with identity matrix
 */
         for (; j < N4; j++, c += N4)
         {
            for (i=0; i < N4; i++)
               c[i] = ATL_rzero;
            c[j] = ATL_rone;
         }
      }
      else  /* matrix is currently lower */
      {
         TYPE *c=a+nr*(N4+1);
/*
 *       Transpose Lower matrix to Upper; padding goes at top of Upper matrix
 */
         trL2U(N, A, lda, c, N4);
/*
 *       Invert diagonal elements
 */
         if (UNIT)
            for (j=nr; j < N4; j++, c += N4+1)
               *c = ATL_rone;
         else
            for (j=nr; j < N4; j++, c += N4+1)
               *c = ATL_rone / *c;
            
/* 
 *       If N != N4, then we must pad the matrix
 */
         if (nr)
         {
            c = a;
/*
 *          Pad first nr cols with identity matrix
 */
            for (j=0; j < nr; j++, c += N4)
            {
               for (i=0; i < N4; i++)
                  c[i] = ATL_rzero;
               c[j] = ATL_rone;
            }
/*
 *          Pad first nr rows of remaining columns with zeros
 */
            for (; j < N4; j++, c += N4)
            {
               for (i=0; i < nr; i++)
                  c[i] = ATL_rzero;
            }
         }
      }
      UPPER = !UPPER;  /* transposition swaps UPLO setting */
   }
/*
 * a now contains correctly padded A^T, so now loop over Mc rows of RHS;
 * Mc is set to a value near 32 so we can get some TLB reuse when doing
 * the transpose of the RHS vectors from row-access to column-access
 */
   b = (UPPER) ? w + nr : w;
   for (i=0; i < M; i += Mc, B += Mc)
   {
      const int mr = Mmin(M-i, Mc);
      Mjoin(PATL,gemoveT)(N, mr, alpha, B, ldb, b, N4);
      if (UPPER)
      {
         if (nr)
            Mjoin(PATL,gezero)(nr, mr, w, N4);
         ATL_trsmRUN(N, mr, a, w);
      }
      else
      {
         if (nr)
            Mjoin(PATL,gezero)(nr, mr, w+N, N4);
         ATL_trsmRLN(N, mr, a, w);
      }
      Mjoin(PATL,gemoveT)(mr, N, ATL_rone, b, N4, B, ldb);
   }
   free(vp);
   return(0);
}
